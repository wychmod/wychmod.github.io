# 多进程、多线程和线程池编程
[toc]
## python中的GIL
1. **gil global interpreter lock（cpython）全局解释器锁**
2. **python中一个线程对应于c语言中的一个线程**
3. **gil时的同一时刻只有一个线程在一个cpu上执行字节码，无法将多个线程映射到多个cpu上执行**
4. **gil会根据执行的字节码行数以及时间片释放gil，gil在遇到io操作时主动释放**
```python
import threading
total = 0


def add():
    global total
    for i in range(1000000):
        total += 1


def desc():
    global total
    for i in range(1000000):
        total -= 1


thread1 = threading.Thread(target=add)
thread2 = threading.Thread(target=desc)

thread1.start()
thread2.start()

thread1.join()
thread2.join()

print(total)
```
> python默认解释器是cPython，有GIL锁所以同一时刻只有一个线程在一个cpu上执行字节码，在后面一些新开发的解释器（PyPy采用JIT技术，对Python代码进行动态编译）中可以做到更好的利用CPU。

## python多线程编程-Threading
**对于io操作来说，多线程和多进程性能差别不大**
### 1.通过Thread类实例化
```
import time
import threading


def get_detail_html(url):
    print('get detail html started')
    time.sleep(2)
    print('get detail html end')


def get_detail_url(url):
    print('get detail url started')
    time.sleep(2)
    print('get detail url end')


if __name__ == '__main__':
    thread1 = threading.Thread(target=get_detail_html, args=('',))
    thread2 = threading.Thread(target=get_detail_url, args=('',))

    # 将thread1和thread2设置为守护线程，即主线程退出，子线程关闭
    thread1.setDaemon(True)
    thread2.setDaemon(True)

    start_time = time.time()
    # 启动线程
    thread1.start()
    thread2.start()

    # 阻塞，等待thread1和thread2两个子线程执行完成
    thread1.join()
    thread2.join()

    print('last time: {}'.format(time.time() - start_time))
```

### 2.通过继承Thread来实现多线程
```
import time
import threading


class GetDetailHtml(threading.Thread):
    def __init__(self, name):
        super().__init__(name=name)

    def run(self):
        print('get detail html started')
        time.sleep(2)
        print('get detail html end')


class GetDetailUrl(threading.Thread):
    def __init__(self, name):
        super().__init__(name=name)

    def run(self):
        print('get detail url started')
        time.sleep(2)
        print('get detail url end')


if __name__ == '__main__':
    thread1 = GetDetailHtml('get_detail_html')
    thread2 = GetDetailUrl('get_detail_url')

    start_time = time.time()

    thread1.start()
    thread2.start()

    thread1.join()
    thread2.join()
    print('last time: {}'.format(time.time() - start_time))
```

## 线程间通信-Queue
### 1.线程通信方式--共享变量
**缺点，共享变量需要加锁，来达到我们想要的效果**
```
import time
import threading
detail_url_list = []


def get_detail_html(detail_url_list):
    # 使用共享变量
    # global detail_url_list
    while True:
        if len(detail_url_list):
            url = detail_url_list.pop()
            # 爬取文章详情页
            print('get detail html started')
            time.sleep(2)
            print('get detail html end')


def get_detail_url(detail_url_list):
    while True:
        # 使用共享变量
        # global detail_url_list
        # 爬取文章列表页
        print('get detail url started')
        time.sleep(2)
        for i in range(20):
            detail_url_list.append('http://projectsedu.com/{id}'.format(id=i))
        print('get detail url end')


# 1.线程通信方式-共享变量
if __name__ == '__main__':
    start_time = time.time()
    thread_detail_url = threading.Thread(target=get_detail_url, args=(detail_url_list,))
    thread_detail_url.start()
    for i in range(10):
        thread_detail_html = threading.Thread(target=get_detail_html, args=(detail_url_list,))
        thread_detail_html.start()

    print('last time: {}'.format(time.time() - start_time))
```
### 2.通过queue的方式进行线程间的通信
因为详情页的数量很多，所以要开启多个线程去爬取详细页，当其中一个爬取请求的时候在等待放回的过程中，gil会释放，另一个线程进行爬取。同时queue的get()是线程保护的，不会两个线程取出同一个变量。
```
# 2.通过queue的方式进行线程间的通信
from queue import Queue
import time
import threading
detail_url_list = []


def get_detail_html(queue):
    while True:
        # 队列的get方法是一个阻塞的函数，即如果队列为空，就阻塞
        url = queue.get()
        # 爬取文章详情页
        print('get detail html started')
        time.sleep(2)
        print('get detail html end')


def get_detail_url(queue):
    while True:
        # 爬取文章列表页
        print('get detail url started')
        time.sleep(2)
        for i in range(20):
            # 向队列里面插入数据
            # put也是一个阻塞函数，当队列已满的时候，会阻塞
            queue.put('http://projectsedu.com/{id}'.format(id=i))
        print('get detail url end')


# 1.线程通信方式-共享变量
if __name__ == '__main__':
    # 最好设置一个最大值，不然太大了，回对内存有影响
    detail_url_queue = Queue(maxsize=1000)
    start_time = time.time()
    thread_detail_url = threading.Thread(target=get_detail_url, args=(detail_url_queue,))
    thread_detail_url.start()
    for i in range(10):
        thread_detail_html = threading.Thread(target=get_detail_html, args=(detail_url_queue,))
        thread_detail_html.start()

    print('last time: {}'.format(time.time() - start_time))
```

> queue 是通过dequeue实现的，deque是双端队列，有线程保护的。

## 线程同步（Lock、RLock、Semaphores、Condition）
首先分析第一标题的时候为什么会出现数字不为0的情况。
```
def add1(a):
    a += 1

def desc1(a):
    a -= 1

"""
1.load a
2.load 1
3.+(-)
4.赋值给a
"""
print(dis.dis(add1))
print(dis.dis(desc1))
```
**加或减的时候，如果在第一步和第四步切换线程，那么假如0+1=1 最后赋值的时候a=1但是切换过去以后a=0-1=-1就会导致没有加成功。**

1. **用锁会影响性能**
2. **锁会引起死锁**
    
    **资源竞争A需要ab，B需要ba，俩个一人拿了一个 。**

    A(a、b)
    acquire (a)
    acquire (b)
    
    B(a、b)
    acquire (a)
    acquire (b)
    
### Lock锁
**锁里不能加锁。**
```
import dis
import threading
from threading import Lock
total = 0
lock = Lock()


def add():
    global total
    global lock
    for i in range(1000000):
        # 获取锁
        lock.acquire()
        total += 1
        # 释放锁
        lock.release()


def desc():
    global total
    global lock
    for i in range(1000000):
        # 获取锁
        lock.acquire()
        total -= 1
        # 释放锁
        lock.release()


thread1 = threading.Thread(target=add)
thread2 = threading.Thread(target=desc)

thread1.start()
thread2.start()

thread1.join()
thread2.join()

print(total)
```

### RLock 可重入的锁
**RLock，在同一个线程里面，可以连续调用多次acquire，一定要注意acquire的次数要和release的次数相等**
```
import threading
# 线程间还是有相互竞争的关系
from threading import RLock
total = 0
lock = RLock()


def add():
    global total
    global lock
    for i in range(1000000):
        # 获取锁
        lock.acquire()
        lock.acquire()
        total += 1
        # 释放锁
        lock.release()
        lock.release()


def desc():
    global total
    global lock
    for i in range(1000000):
        # 获取锁
        lock.acquire()
        total -= 1
        # 释放锁
        lock.release()


thread1 = threading.Thread(target=add)
thread2 = threading.Thread(target=desc)

thread1.start()
thread2.start()

thread1.join()
thread2.join()

print(total)
```
## 线程同步--conditon 使用以及源码分析
**条件变量，用于复杂的线程间同步**
```
# 通过condition完成协同读诗
from threading import Lock
from threading import Condition
import threading


class XiaoAi(threading.Thread):
    def __init__(self, cond):
        super().__init__(name='小爱')
        self.cond = cond

    def run(self):
        with self.cond:
            # 等待
            self.cond.wait()
            print('{} : 在'.format(self.name))
            # 通知
            self.cond.notify()
            # 等待
            self.cond.wait()
            print('{} : 好啊'.format(self.name))
            # 通知
            self.cond.notify()


class TianMao(threading.Thread):
    def __init__(self, cond):
        super().__init__(name='天猫')
        self.cond = cond

    def run(self):
        # 建议使用with语句，不然acquire和release一定要成对出现
        with self.cond:
            print('{} : 小爱同学'.format(self.name))
            # 通知
            self.cond.notify()
            # 等待
            self.cond.wait()
            print('{} : 我们来对古诗吧'.format(self.name))
            # 通知
            self.cond.notify()
            # 等待
            self.cond.wait()


if __name__ == '__main__':
    cond = threading.Condition()
    xiao_ai = XiaoAi(cond)
    tian_mao = TianMao(cond)

    # 启动的先后顺序很重要
    # 在调用with方法，或者acquire方法之后，再调用wait和notify方法
    xiao_ai.start()
    tian_mao.start()

    # 先启动tian_mao，会打印出‘小爱同学’，但程序会‘死掉’，这是因为self.cond.notify()通知已经发出去了
    # 但是后面启动xiao_ai，xiao_ai的self.cond.wait()收不到通知。
```
> condition有两层锁， 一把底层锁会在线程调用了wait方法的时候释放， 上面的锁会在每次调用wait的时候分配一把并放入到cond的等待队列（deque实现的）中，等到notify方法的唤醒。

## 线程同步--Semaphore 信号量（是用于控制进入数量的锁）
**Semaphore 是用于控制进入数量的锁**
```
# 文件、读、写、写一般只是用于一个线程写，读可以允许有多个

# 做爬虫
import threading
import time


class HtmlSpider(threading.Thread):
    def __init__(self, url, sem):
        super().__init__()
        self.url = url
        self.sem = sem

    def run(self):
        time.sleep(2)
        print('got html text success')
        self.sem.release()


class UrlProducer(threading.Thread):
    def __init__(self, sem):
        super().__init__()
        self.sem = sem

    def run(self):
        for i in range(20):
            # 每次调用Semaphore的acquire方法，sem = threading.Semaphore(3)设置的次数都会减一
            self.sem.acquire()
            html_thread = HtmlSpider('http://www.baidu.com/{}'.format(i), self.sem)
            html_thread.start()


if __name__ == '__main__':
    # 一次允许3个并发
    sem = threading.Semaphore(3)
    url_producer = UrlProducer(sem)
    url_producer.start()
```
> Semaphore底层是用Condition实现的

## concurrent线程池编码
### 线程池
```
from concurrent.futures import ThreadPoolExecutor
import time

# 线程池
# 主线程中可以获取某一个线程的状态或者某一个任务的状态，以及返回值
# 当一个线程完成的时候，我们主线程能立即知道
# futures可以让多线程和多进程编码接口一致


def get_html(times):
    time.sleep(times)
    print('get page {} success'.format(times))
    return times


executor = ThreadPoolExecutor(max_workers=1)
# 通过submit函数提交执行的函数到线程池中，submit是立即返回，是非阻塞的函数
# submit(fn, *args, **kwargs)
task1 = executor.submit(get_html, (3))
task2 = executor.submit(get_html, (2))

# done方法用判断某个任务是否完成
print(task1.done())
# cancel不能取消任务状态为执行中或者执行完成的task
print(task2.cancel())
time.sleep(3)
print(task1.done())

# result方法可以获取task的执行结果
print(task1.result())
```

### as_completed
**获取已经成功的task的返回，是个生成器**
```
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from collections.abc import Iterable


# 线程池
# 主线程中可以获取某一个线程的状态或者某一个任务的状态，以及返回值
# 当一个线程完成的时候，我们主线程能立即知道
# futures可以让多线程和多进程编码接口一致


def get_html(times):
    time.sleep(times)
    print('get page {} success'.format(times))
    return times


executor = ThreadPoolExecutor(max_workers=2)
# 通过submit函数提交执行的函数到线程池中，submit是立即返回，是非阻塞的函数
# submit(fn, *args, **kwargs)
# task1 = executor.submit(get_html, (3))
# task2 = executor.submit(get_html, (2))

# 获取已经成功的task的返回
# 批量提交，利用列表推导式
urls = [3, 2, 4]
all_task = [executor.submit(get_html, (url)) for url in urls]

for future in as_completed(all_task):
    data = future.result()
    print('get {} page'.format(data))
```
### map
**通过executor获取已经完成的task,map的返回顺序是和urls一样的**
```
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
from collections.abc import Iterable


# 线程池
# 主线程中可以获取某一个线程的状态或者某一个任务的状态，以及返回值
# 当一个线程完成的时候，我们主线程能立即知道
# futures可以让多线程和多进程编码接口一致


def get_html(times):
    time.sleep(times)
    print('get page {} success'.format(times))
    return times


executor = ThreadPoolExecutor(max_workers=2)
urls = [3, 2, 4]

# 通过executor获取已经完成的task，executor.map返回的是future.result()
# map的返回顺序是和urls一样的
for data in executor.map(get_html, urls):
    print('get {} page'.format(data))
```

### wait() 阻塞主线程
**等待某一或一组任务的完成，阻塞主线程**
```
from concurrent.futures import ThreadPoolExecutor, wait
import time
from collections.abc import Iterable


# 线程池
# 主线程中可以获取某一个线程的状态或者某一个任务的状态，以及返回值
# 当一个线程完成的时候，我们主线程能立即知道
# futures可以让多线程和多进程编码接口一致


def get_html(times):
    time.sleep(times)
    print('get page {} success'.format(times))
    return times


executor = ThreadPoolExecutor(max_workers=2)
# 通过submit函数提交执行的函数到线程池中，submit是立即返回，是非阻塞的函数
# submit(fn, *args, **kwargs)
# task1 = executor.submit(get_html, (3))
# task2 = executor.submit(get_html, (2))

# 获取已经成功的task的返回
# 批量提交，利用列表推导式
urls = [3, 2, 4]
all_task = [executor.submit(get_html, (url)) for url in urls]

# 等待某一或一组任务的完成，阻塞主线程
wait(all_task)
print('ok')
```

### Future
**未来对象，task的返回容器,记录了Future的状态**

**常用方法**：

![image](../../youdaonote-images/BA0B8D6D3819499DBD07139133BB75CA.png)

## 多线程和多进程的比较
**消耗cpu的操作，用多进程编程，对于io操作来说，使用多线程编程，进程切换代价要高于线程**

多进程：
```
from concurrent.futures import ThreadPoolExecutor, as_completed
from concurrent.futures import ProcessPoolExecutor
import time

# 消耗cpu的操作，用多进程编程，对于io操作来说，使用多线程编程，进程切换代价要高于线程

# 1.对于耗费cpu的操作，计算


def fib(n):
    if n <= 2:
        return 1
    return fib(n-1) + fib(n-2)


# 在windows下，多进程编程一定需要放在if __name__ == '__main__':中，linux中不存在这个问题
if __name__ == '__main__':
    # 实现了__enter__ 和 __exit__
    # 多线程
    with ThreadPoolExecutor(max_workers=3) as executor:
        all_task = [executor.submit(fib, (num)) for num in range(25, 35)]
        start_time = time.time()
        for future in as_completed(all_task):
            data = future.result()
            print('exe result {} '.format(data))
        thread_last_time = time.time() - start_time
        print('last time is {} '.format(thread_last_time))

    # 多进程
    with ProcessPoolExecutor(max_workers=3) as executor:
        all_task = [executor.submit(fib, (num)) for num in range(25, 35)]
        start_time = time.time()
        for future in as_completed(all_task):
            data = future.result()
            print('exe result {} '.format(data))
        process_last_time = time.time() - start_time
        print('last time is {} '.format(process_last_time))

    print('多进程比多线程快 {} 秒'.format(thread_last_time - process_last_time))
```

多线程：
```
from concurrent.futures import ThreadPoolExecutor, as_completed
from concurrent.futures import ProcessPoolExecutor
import time

# 消耗cpu的操作，用多进程编程，对于io操作来说，使用多线程编程，进程切换代价要高于线程

# 2.对于io操作来说，多线程优于多进程


def random_sleep(n):
    time.sleep(n)
    return n


# 在windows下，多进程编程一定需要放在if __name__ == '__main__':中，linux中不存在这个问题
if __name__ == '__main__':
    # 实现了__enter__ 和 __exit__
    # 多线程
    with ThreadPoolExecutor(max_workers=3) as executor:
        all_task = [executor.submit(random_sleep, (n)) for n in [2]*30]
        start_time = time.time()
        for future in as_completed(all_task):
            data = future.result()
            print('exe result {} '.format(data))
        thread_last_time = time.time() - start_time
        print('last time is {} '.format(thread_last_time))

    # 多进程
    with ProcessPoolExecutor(max_workers=3) as executor:
        all_task = [executor.submit(random_sleep, (n)) for n in [2]*30]
        start_time = time.time()
        for future in as_completed(all_task):
            data = future.result()
            print('exe result {} '.format(data))
        process_last_time = time.time() - start_time
        print('last time is {} '.format(process_last_time))

    print('多线程比多进程快 {} 秒'.format(process_last_time - thread_last_time))
```
> 对于耗费cpu的操作，例如计算。多进程优于多线程。对于io操作，多线程优于多进程。

## 多进程编程-multiprocessing
```
import os
import time
# fork只能用于linux/unix中
pid = os.fork()
print('zy')
if pid == 0:
    # os.getpid()当前进程的id，os.getppid()当前进程的父进程的id
    # 子进程会把父进程所有的数据拷贝一份，fork()之后的代码会重新运行一遍
    print('子进程 {} ， 父进程是：{} .'.format(os.getpid(), os.getppid()))
else:
    print('我是父进程：{}'.format(pid))

# 父进程运行完成就退出，子进程不会退出
time.sleep(2)
```

### multiprocessing.Process进程和multiprocessing.Pool进程池
```
import multiprocessing
import time


# 多进程编程
def get_html(n):
    time.sleep(n)
    print('sub progress success')
    return n


if __name__ == '__main__':
    progress = multiprocessing.Process(target=get_html, args=(2,))
    print(progress.pid)
    progress.start()
    print(progress.pid)
    progress.join()
    print('main progress end')

    # 使用进程池
    pool = multiprocessing.Pool(multiprocessing.cpu_count())
    # 提交任务
    result = pool.apply_async(get_html, args=(3,))

    # 关闭pool，不再接收新的任务
    pool.close()
    # 等待所有任务完成
    pool.join()
    print(result.get())
```

### imap 类似于线程中的map
```
import multiprocessing
import time


# 多进程编程
def get_html(n):
    time.sleep(n)
    print('sub progress success')
    return n


if __name__ == '__main__':
    # 使用进程池
    pool = multiprocessing.Pool(multiprocessing.cpu_count())

    # imap，按照顺序打印
    for result in pool.imap(get_html, [1, 5, 3]):
        print('{} sleep suceess'.format(result))
```

### imap_unordered 哪个先执行完打印哪个
```
import multiprocessing
import time


# 多进程编程
def get_html(n):
    time.sleep(n)
    print('sub progress success')
    return n


if __name__ == '__main__':
    # 使用进程池
    pool = multiprocessing.Pool(multiprocessing.cpu_count())

    # imap，哪个先执行完打印哪个
    for result in pool.imap_unordered(get_html, [1, 5, 3]):
        print('{} sleep suceess'.format(result))
```

## 进程间通信--Queue、Pipe、Manager
### Queue
**利用Queue，这个Queue不是queue里面的**
```
from multiprocessing import Process, Queue
import time


def producer(queue):
    queue.put('a')
    time.sleep(2)


def consumer(queue):
    time.sleep(2)
    data = queue.get()
    print(data)


if __name__ == '__main__':
    queue = Queue(10)
    my_producer = Process(target=producer, args=(queue,))
    my_consumer = Process(target=consumer, args=(queue,))
    my_producer.start()
    my_consumer.start()
    my_producer.join()
    my_consumer.join()
```
> 共享全局变量不能适用于多进程编程，只能用于多线程编程  

### Manager().Queue()
**multiprocessing里面的Queue不能用于pool进程池，但是multiprocessing里面有一个Manager**
```
from multiprocessing import Process, Queue, Manager, Pool
import time


def producer(queue):
    queue.put('a')
    time.sleep(2)


def consumer(queue):
    time.sleep(2)
    data = queue.get()
    print(data)


if __name__ == '__main__':
    pool = Pool(2)
    queue = Manager().Queue(10)

    pool.apply_async(producer, args=(queue,))
    pool.apply_async(consumer, args=(queue,))

    pool.close()
    pool.join()
```
>  pool中的进程间通信用manager中的Queue

### pipe
**通过pipe实现进程间通信**
```
# 通过pipe实现进程间通信
# pipe的性能高于queue
from multiprocessing import Pipe, Pool, Process
import time


def producer(pipe):
    pipe.send('zy')
    time.sleep(2)


def consumer(pipe):
    print(pipe.recv())


if __name__ == '__main__':
    pool = Pool(2)
    # pipe只能适用于两个进程间的通信
    recevie_pipe, send_pipe = Pipe()
    my_producer = Process(target=producer, args=(send_pipe,))
    my_consumer = Process(target=consumer, args=(recevie_pipe,))

    my_producer.start()
    my_consumer.start()

    my_producer.join()
    my_consumer.join()
```
> pipe只能适用于两个进程间的通信

### Manager().dict() 共享数据结构
```
def add_data(p_dict, key, value):
    p_dict[key] = value

if __name__ == "__main__":
    progress_dict = Manager().dict()

    first_progress = Process(target=add_data, args=(progress_dict, "bobby1", 22))
    second_progress = Process(target=add_data, args=(progress_dict, "bobby2", 23))

    first_progress.start()
    second_progress.start()
    first_progress.join()
    second_progress.join()

    print(progress_dict)
```