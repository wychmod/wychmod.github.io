# äººå·¥æ™ºèƒ½-æ•°æ®é¢„å¤„ç†-7
[toc]
## æ•°æ®é¢„å¤„ç†çš„æ„ä¹‰
- è¦åˆ†æå¤„ç†çš„æ•°æ®éœ€è¦ä¸€å®šçš„è´¨é‡
- æ•°æ®è´¨é‡åŒ…æ‹¬ä¸‹é¢å‡ ä¸ªå› ç´ ï¼š
    1. æ­£ç¡®æ€§ï¼ˆCorrectnessï¼‰
    2. ä¸€è‡´æ€§ï¼ˆConsistencyï¼‰
    3. å®Œæ•´æ€§ï¼ˆCompletenessï¼‰ 
    4. å¯é æ€§ï¼ˆReliabilityï¼‰
- ç°å®ä¸–ç•Œä¸­çš„æ•°æ®å¾€å¾€æ˜¯è¿™æ ·çš„ï¼š
    1. ä¸å®Œæ•´çš„ï¼šç¼ºå°‘å±æ€§å€¼æˆ–æŸäº›æ„Ÿå…´è¶£çš„å±æ€§ï¼Œæˆ–ä»…åŒ…å«èšé›†æ•°æ®ã€‚
    2. å«å™ªå£°çš„ï¼šåŒ…å«é”™è¯¯æˆ–å­˜åœ¨åç¦»å‡å€¼çš„ç¦»ç¾¤å€¼ã€‚
    3. ä¸ä¸€è‡´çš„ï¼šé‡‡ç”¨çš„ç¼–ç æˆ–è¡¨ç¤ºä¸åŒï¼Œå¦‚å±æ€§åç§°ä¸åŒ
    4. å†—ä½™çš„ï¼šå¦‚å±æ€§ä¹‹é—´å¯ä»¥ç›¸äº’å¯¼å‡º

## æ•°æ®é”™è¯¯çš„ä¸å¯é¿å…æ€§
- æ•°æ®è¾“å…¥å’Œè·å¾—è¿‡ç¨‹æ•°æ®é”™è¯¯
- æ•°æ®é›†æˆæ‰€è¡¨ç°å‡ºæ¥çš„é”™è¯¯
- æ•°æ®ä¼ è¾“è¿‡ç¨‹æ‰€å¼•å…¥çš„é”™è¯¯ 
- æ®ç»Ÿè®¡æœ‰é”™è¯¯çš„æ•°æ®å æ€»æ•°æ®çš„5%å·¦å³

## æ•°æ®é”™è¯¯çš„ç¾éš¾
- é”™è¯¯çš„ç»“æœ
- å¯èƒ½æ¶ˆè€—æ›´å¤šçš„èµ„æº
- å¯¼è‡´é”™è¯¯çš„å†³ç­–
- æ²¡æœ‰æ­ç¤ºæ•°æ®çœŸå®çš„è§„å¾‹

## æ•°æ®é¢„å¤„ç†
- æœºå™¨å­¦ä¹ ç®—æ³•æœ€ç»ˆå­¦ä¹ ç»“æœçš„ä¼˜åŠ£å–å†³äºä¸¤ä¸ªä¸»è¦å› ç´ ï¼š==æ•°æ®çš„è´¨é‡==å’Œ==æ•°æ®ä¸­è•´å«çš„æœ‰ç”¨ä¿¡æ¯çš„æ•°é‡==ã€‚
- å› æ­¤ï¼Œåœ¨å°†æ•°æ®é›†åº”ç”¨äºå­¦ä¹ ç®—æ³•ä¹‹å‰ï¼Œå¯¹å…¶è¿›è¡Œæ£€éªŒåŠé¢„å¤„ç†æ˜¯è‡³å…³é‡è¦çš„ã€‚
- æ„å»ºä¼˜è´¨è®­ç»ƒé›†è¿›è¡Œçš„æ•°æ®é¢„å¤„ç†ï¼š
    1. å¤„ç†ç¼ºå¤±æ•°æ®
    2. å¤„ç†ç±»åˆ«æ•°æ®
    3. åˆ’åˆ†æ•°æ®é›†
    4. æ•°æ®æ ‡å‡†åŒ–ä¸å½’ä¸€åŒ–
    5. ç‰¹å¾é€‰æ‹©
    6. ç‰¹å¾æ’åº

## å¤„ç†ç¼ºå¤±æ•°æ®
### ä»€ä¹ˆæ˜¯ç¼ºå¤±æ•°æ®
- åœ¨å®é™…åº”ç”¨è¿‡ç¨‹ä¸­ï¼Œæ ·æœ¬ç”±äºå„ç§åŸå› ç¼ºå°‘ä¸€ä¸ªæˆ–å¤šä¸ªå€¼çš„æƒ…å†µå¹¶ä¸å°‘è§ã€‚
- å…¶åŸå› ä¸»è¦æœ‰ï¼šæ•°æ®é‡‡é›†è¿‡ç¨‹ä¸­å‡ºç°äº†é”™è¯¯ï¼Œå¸¸ç”¨çš„åº¦é‡æ–¹æ³•ä¸é€‚ç”¨äºæŸäº›ç‰¹å¾ï¼Œæˆ–è€…åœ¨è°ƒæŸ¥è¿‡ç¨‹ä¸­æŸäº›æ•°æ®æœªè¢«å¡«å†™ï¼Œç­‰ç­‰ã€‚
- ![image](../../youdaonote-images/25D80EA747FE4CAA9AF8EA4402F49B6A.png)
- é€šå¸¸ï¼Œæˆ‘ä»¬è§åˆ°çš„ç¼ºå¤±å€¼æ˜¯æ•°æ®è¡¨ä¸­çš„ç©ºå€¼ï¼Œæˆ–è€…æ˜¯ç±»ä¼¼äºNaNï¼ˆNot ANumberï¼Œéæ•°å­—ï¼‰çš„å ä½ç¬¦ã€‚
å¦‚æœæˆ‘ä»¬å¿½ç•¥è¿™äº›ç¼ºå¤±å€¼ï¼Œå°†å¯¼è‡´å¤§éƒ¨åˆ†çš„è®¡ç®—å·¥å…·æ— æ³•å¯¹åŸå§‹æ•°æ®è¿›è¡Œå¤„ç†ï¼Œæˆ–è€…å¾—åˆ°æŸäº›ä¸å¯é¢„çŸ¥çš„ç»“æœã€‚å› æ­¤ï¼Œåœ¨åšæ›´æ·±å…¥çš„åˆ†æä¹‹å‰ï¼Œå¿…é¡»å¯¹è¿™äº›ç¼ºå¤±å€¼è¿›è¡Œå¤„ç†ã€‚

### å¤„ç†ç¼ºå¤±æ•°æ®ä»£ç 
```
from IPython.display import Image
import pandas as pd
from io import StringIO
import sys


## å¤„ç†ç¼ºå¤±å€¼


# In[1]:
# æœ‰ç¼ºå¤±æ•°æ®é›†çš„ç®€å•ä¾‹å­
# ä¸€èˆ¬æ¥è¯´ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œä¸€åˆ—è¡¨ç¤ºæ ·æœ¬çš„ä¸€ä¸ªç‰¹å¾
csv_data = '''A,B,C,D
1.0,2.0,3.0,4.0
5.0,6.0,,8.0
10.0,11.0,12.0,'''

# æ˜¾ç¤ºæ•°æ®
df = pd.read_csv(StringIO(csv_data))
# ä»¥è¡¨æ ¼æ˜¾ç¤ºæ˜¾ç¤º
df

# In[2]:
# ä¸æ˜¾ç¤ºè¡¨å¤´ï¼Œåªæ˜¯æ˜¾ç¤ºé‡Œé¢çš„äºŒç»´æ•°æ®
df.values

# In[3]:
# ç»Ÿè®¡æ¯ä¸€åˆ—ç¼ºå¤±æ•°æ®ä¸ªæ•°
df.isnull().sum()

# In[4]:
# å¦‚æœæ•°æ®é‡ç‰¹åˆ«å¤§çš„æ—¶å€™ï¼Œå¯ä»¥é€šè¿‡åˆ é™¤çš„æ–¹å¼ï¼Œå› ä¸ºå¤§æ•°æ®æ—¶ä»£ï¼Œ
# å¾ˆå¤šæ—¶å€™æŸå¤±å°‘é‡æ•°æ®å¹¶ä¸è¦ç´§ã€‚

# åˆ é™¤åŒ…å«ç¼ºå¤±æ•°æ®çš„è¡Œ
df.dropna(axis=0)

# In[5]:
# åˆ é™¤åŒ…å«ç¼ºå¤±æ•°æ®çš„åˆ—
df.dropna(axis=1)

# In[6]:
# æ–°çš„æµ‹è¯•æ•°æ®
csv_data2 = '''A,B,C,D
1.0,2.0,,
,,,
10.0,11.0,12.0,'''

# æ˜¾ç¤ºæ•°æ®
df2 = pd.read_csv(StringIO(csv_data2))
# ä»¥è¡¨æ ¼æ˜¾ç¤ºæ˜¾ç¤º
df2

# In[7]:
# å…¶ä»–çš„å¤„ç†æ–¹æ³•
# å¦‚æœä¸€è¡Œå…¨æ˜¯NaNæ‰åˆ é™¤
df2.dropna(how='all')  

# In[8]:
# thresh=3,é‚£ä¹ˆä¸€è¡Œå½“ä¸­è‡³å°‘æœ‰ä¸‰ä¸ªæ•°å€¼æ—¶æ‰å°†å…¶ä¿ç•™
df2.dropna(axis=0,thresh=3)

# In[9]:
# ç¼ºçœæ˜¯å¯¹è¡Œè¿›è¡Œå¤„ç†ã€‚thresh=3,é‚£ä¹ˆä¸€è¡Œå½“ä¸­è‡³å°‘æœ‰ä¸‰ä¸ªæ•°å€¼æ—¶æ‰å°†å…¶ä¿ç•™
df2.dropna(thresh=3)

# In[10]:
# thresh=2,é‚£ä¹ˆä¸€åˆ—å½“ä¸­è‡³å°‘æœ‰ä¸¤ä¸ªæ•°å€¼æ—¶æ‰å°†å…¶ä¿ç•™
df2.dropna(axis=1,thresh=2)

# In[11]:
# only drop rows where NaN appear in specific columns (here: 'C')
# åªæ˜¯å°†Cåˆ—ä¸­æœ‰NaNçš„è¡Œåˆ é™¤æ‰
df2.dropna(subset=['C'])

# In[12]:
# é‡æ–°æ˜¾ç¤ºç¬¬ä¸€ç»„æ•°æ®é›†
df.values

# In[13]:
# å¦‚æœæ•°æ®é‡æ¯”è¾ƒå°‘ï¼Œæˆ–è€…ä¸æƒ³åˆ é™¤ï¼Œé€šè¿‡ä½¿ç”¨å…¶ä»–åˆ—å€¼æ±‚å¹³å‡çš„æ–¹å¼
# æ³¨æ„è¿™æ˜¯é’ˆå¯¹è¿ç»­æ•°å€¼æ•°æ®çš„å¤„ç†æ–¹æ³•ï¼Œå¦‚æœæ˜¯ç¦»æ•£å€¼ï¼Œä¸€èˆ¬æ˜¯è®¾ç½®æ•°é‡æœ€å¤š/é¢‘æ•°æœ€é«˜çš„ç¦»æ•£å€¼
# æ¯”å¦‚æŸä¸ªå±æ€§çš„ç¦»æ•£å€¼æœ‰3ä¸ªï¼Œ1æœ‰5ä¸ªï¼Œ2æœ‰3ä¸ªï¼Œ3æœ‰1ä¸ªï¼Œé‚£ä¹ˆè¿™ä¸ªå±æ€§çš„ç¼ºå¤±å€¼å°±è®¾ç½®ä¸º1.
# ç»Ÿè®¡çš„æ–¹æ³•æ¯”è¾ƒå¤šï¼Œæ­¤å¤„ç•™ä¸€ä¸ªä½œä¸šè‡ªå·±å®ç°ã€‚
from sklearn.impute import SimpleImputer

imr = SimpleImputer(strategy='mean', verbose=0)
imr = imr.fit(df.values)
imputed_data = imr.transform(df.values)
imputed_data
```

## å¤„ç†ç±»åˆ«æ•°æ®
### ä»€ä¹ˆæ˜¯ç±»åˆ«æ•°æ®ï¼Ÿ
- æœºå™¨å­¦ä¹ è®¡ç®—å¤„ç†çš„æ—¶å€™ï¼Œåªèƒ½å¤„ç†æ•°å€¼ï¼
- ç°å®çš„æ•°æ®ä¸­ï¼Œå¾€å¾€æœ‰å¾ˆå¤šéæ•°å€¼æ•°æ®ã€‚
![image](../../youdaonote-images/A1495E7AEBB04245B8AEFAF3E5C318A8.png)

### å¦‚ä½•å¤„ç†ç±»åˆ«æ•°æ®ï¼Ÿ
- å°†ç±»åˆ«æ•°æ®ç›´æ¥æ˜ å°„åˆ°ä¸€ä¸ªæ•´æ•°ï¼š
    1. size(è¡£æœå°ºå¯¸)ï¼šå¯ä»¥æ˜ å°„åˆ°1ï¼Œ2ï¼Œ3ï¼Œå› ä¸ºå°ºå¯¸æœ‰å¤§å°ä¹‹åˆ†ï¼Œå¯ä»¥æ¯”è¾ƒçš„ï¼Œæ•°å€¼çš„å¤§å°å¯ä»¥è¡¨ç°è¿™ä¸ªç‰¹æ€§ã€‚
    2. classlabelï¼ˆæ ·æœ¬æ‰€å±ç±»åˆ«ï¼‰ï¼šå½“åªæœ‰ä¸¤ç±»ï¼Œå¯ä»¥ç”¨0å’Œ1è¡¨ç¤º
    3. ==é—®é¢˜æ¥äº†==ï¼šå½“æœ‰å¤šç±»ï¼ˆè¶…è¿‡ä¸¤ç±»ï¼‰æ²¡æœ‰å¤§å°åŒºåˆ†æ•°æ®çš„æ—¶å€™ï¼Œæ¯”å¦‚colorï¼ˆè¡£æœé¢œè‰²ï¼‰ï¼Œçº¢è‰²ï¼ˆredï¼‰ç»¿è‰²ï¼ˆgreenï¼‰å’Œè“è‰²ï¼ˆblueï¼‰æ²¡æœ‰è°å¤§è°å°ä¹‹è¯´ï¼Œæ€ä¹ˆåŠï¼Ÿï¼Ÿ

![image](../../youdaonote-images/DE48626E958B47CAAB8D20A308ED32CB.png)
![image](../../youdaonote-images/8A9A9F26106C417A89B32BEFFEBB09DF.png)

### one-hot ç‹¬çƒ­ç¼–ç 
- æ ¹æ®å‡ºç°çš„ç±»åˆ«æ•°é‡åˆ†æˆå¤šä¸ªå±æ€§ï¼Œå¯¹åº”çš„å€¼ç»™1å€¼ï¼Œå…¶ä»–çš„éƒ½æ˜¯0å€¼ã€‚çœ‹ä¸‹é¢é¢œè‰²colorä¾‹å­ã€‚

![image](../../youdaonote-images/59E274FA09AF44B8A8E167D920977CFC.png)
![image](../../youdaonote-images/2614E160B86D4C5296698685EAC3D45C.png)

### å¤„ç†ç±»åˆ«æ•°æ®ä»£ç 
```
## å¤„ç†ç±»åˆ«æ•°æ®

# In[14]:
import pandas as pd

df = pd.DataFrame([['green', 'M', 10.1, 'class1'],
                   ['red', 'L', 13.5, 'class2'],
                   ['blue', 'XL', 15.3, 'class1']])

df.columns = ['color', 'size', 'price', 'classlabel']
# æ˜¾ç¤ºä¸¾ä¾‹æ•°æ®
df_origin = df
df


# In[15]:
# æ˜ å°„sizeæ•°æ®
size_mapping = {'XL': 3,
                'L': 2,
                'M': 1}

df['size'] = df['size'].map(size_mapping)
df

# In[16]:
import numpy as np
# å°†ç±»åˆ«è®¾ç½®ä¸º0å’Œ1
class_mapping = {label: idx for idx, label in enumerate(np.unique(df['classlabel']))}
class_mapping

# In[17]:
# ç±»åˆ«æ•°æ®æ˜ å°„åˆ°æ•´æ•°
df['classlabel'] = df['classlabel'].map(class_mapping)
df

# In[18]:
# ä»…ä»…æ˜¾ç¤ºäºŒç»´ç‰¹å¾æ•°æ®
X = df[['color', 'size', 'price']].values
X

# numpy
# In[19]:
# é¦–å…ˆå°†coloré¢œè‰²æ•°æ®è½¬ä¸ºæ•°å€¼
from sklearn.preprocessing import LabelEncoder
color_le = LabelEncoder()
# åªæ¥å—æ•°ç»„æ•°æ®ï¼Œä¸æ¥å—dfæ•°æ®
X[:, 0] = color_le.fit_transform(X[:, 0])
X

# In[20]:
# å°†coloré¢œè‰²çš„æ•°å€¼è½¬ä¸ºone-hotæ•°æ®
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(categorical_features=[0])
ohe.fit_transform(X).toarray()

# In[21]:
# å†æ¬¡æ˜¾ç¤ºè¡¨æ ¼å†…å®¹æ•°æ®
df

# In[22]:
# ä½¿ç”¨å¦ä¸€ä¸ªæ–¹æ³•è½¬0ne-hotï¼špandasæ¨¡å—çš„get_dummiesæ–¹æ³•
pd.get_dummies(df[['price', 'color', 'size']])
```

## åˆ’åˆ†æ•°æ®é›†
- å¾ˆå¤šæ—¶å€™ï¼Œæˆ‘ä»¬åªæ˜¯æ‹¥æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œå¯¹äºæœªæ¥è¦æµ‹è¯•è¦å‘ç”Ÿçš„æ•°æ®å¹¶ä¸çŸ¥é“ã€‚
- ä¸€èˆ¬æ¥è¯´ï¼Œå¯¹äºä¸€ä¸ªå·²çŸ¥çš„æ•°æ®é›†ï¼Œä¸€éƒ¨åˆ†æ•°æ®ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œè€Œå¦ä¸€éƒ¨åˆ†ç”¨äºæµ‹è¯•æ¨¡å‹æ˜¯å¦æœ‰æ•ˆã€‚
- å¸¸è§çš„åˆ’åˆ†æ¯”ä¾‹æ˜¯8ï¼š2æˆ–è€…7ï¼š3ï¼Œå‰é¢æ˜¯è®­ç»ƒé›†ï¼Œåé¢æ˜¯æµ‹è¯•é›†ã€‚

### åˆ’åˆ†æ•°æ®é›†ä»£ç 
```
# æŸ¥çœ‹å½“å‰è·¯å¾„
import os
os.getcwd()

# In[24]:
# è¯»å–é…’çš„æ•°æ®é›†
data_dir = 'F:/2019-notebook/2017_2018_2/python_code/MTrain/MachineLearn/3_ML/5_Preprocess/'
df_wine = pd.read_csv(data_dir + 'wine.data', header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']
df_wine.tail()

# In[]:
print('Class labels', np.unique(df_wine['Class label']))

# In[25]:
# å°†å·²çŸ¥çš„æ•°æ®åŒ–ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
from sklearn.model_selection import train_test_split

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

X_train, X_test, y_train, y_test =  train_test_split(X, y, 
                     test_size=0.3, 
                     random_state=0, 
                     stratify=y)
X.shape,X_train.shape,X_test.shape
```

## æ•°æ®æ ‡å‡†åŒ–ä¸å½’ä¸€åŒ–
### ä¸ºä½•è¦æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–
- æ ·æœ¬ä¸åŒçš„ç‰¹å¾/å±æ€§æ‰€åœ¨çš„æ•°å€¼èŒƒå›´å·®å¼‚å·¨å¤§ï¼Œå¯¼è‡´è®­ç»ƒä¸æ”¶æ•›æˆ–å…¶ä»–é—®é¢˜
- æ‰€æœ‰æ•°æ®åœ¨ç›¸åŒçš„å–å€¼ç©ºé—´æ›´å®¹æ˜“å¤„ç†ï¼Œæ–¹ä¾¿æ¨¡å‹çš„ç»Ÿä¸€åŒ–å’Œè§„èŒƒåŒ–
- æ›´å®¹æ˜“å‘ç°æ•°æ®çš„æœ¬è´¨è§„å¾‹
![image](../../youdaonote-images/24CD1A83333D41C5B5E111181ABED9BD.png)
> ä¸¤ä¸ªç‰¹å¾çš„æ•°æ®èŒƒå›´å·®è·ç”šå¤§

### å½’ä¸€åŒ–
- **å°†æ•°æ®ç¼©æ”¾åˆ°[0,1]çš„èŒƒå›´**
- **è®¡ç®—æ–¹æ³•ï¼šx = (x-min)/(max-min)**
- ==ç‰¹åˆ«æ³¨æ„ï¼šè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¸€èˆ¬éƒ½è¦å½’ä¸€åŒ–ï¼Œminå’Œmaxä¸€å®šæ˜¯è®­ç»ƒé›†çš„minå’Œmaxï¼Œxåˆ†åˆ«æ˜¯è®­ç»ƒå’Œæµ‹è¯•é›†çš„==
- ==ä»£ç ä¸­ä½“ç°åœ¨æµ‹è¯•é›†åªæœ‰transformæ“ä½œï¼Œè€Œè®­ç»ƒé›†å¤šäº†ä¸€ä¸ªfitæ“ä½œï¼Œè¿™ä¸ªfitæ“ä½œå°±æ˜¯ä»è®­ç»ƒé›†ä¸­å¾—åˆ°minå’Œmaxä¸¤ä¸ªå‚æ•°çš„è¿‡ç¨‹ï¼Œè¿™ä¸ªminå’ŒmaxåŒæ—¶ä¼šç”¨åˆ°
æµ‹è¯•é›†ï¼Œæ‰€ä»¥æµ‹è¯•é›†æ²¡æœ‰è¿™ä¸ªæ­¥éª¤ã€‚==
- ==å¯¹äºä¸‹é¢çš„æ ‡å‡†åŒ–ä¸€æ ·é“ç†ï¼Œä»è®­ç»ƒæ•°æ®ä¸­è®¡ç®—å‡ºå‡å€¼ğœ‡å’Œæ ‡å‡†å·®
ğœï¼Œä¹Ÿåº”ç”¨åˆ°æµ‹è¯•æ•°æ®ã€‚
WHYï¼Ÿå› ä¸ºæˆ‘ä»¬åŸºäºçš„å‡è®¾æ˜¯ï¼šè®­ç»ƒæ•°æ®ä¸æ•°æ®æ•´ä½“çš„åˆ†å¸ƒæ˜¯ä¸€è‡´çš„ï¼Œæ¨¡å‹æ¥è‡ªè®­ç»ƒæ•°æ®ã€‚==
![image](../../youdaonote-images/85029AF856764988B32F673543220375.png)

### æ ‡å‡†åŒ–
- æ•°æ®æ ‡å‡†åŒ–æ–¹æ³•ç»è¿‡å¤„ç†åæ•°æ®ç¬¦åˆæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œå³å‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1
- è½¬åŒ–å‡½æ•°ä¸ºï¼šx =(x - ğœ‡)/ğœ
![image](../../youdaonote-images/5C792372F6894E4BB14F6C1CB9974C7A.png)
> è¿™é‡Œä¸€èˆ¬åšäº†ä¸€ä¸ªå‡è®¾ï¼Œå°±æ˜¯ç‰¹å¾æ•°æ®ï¼ˆæŸä¸ªå±æ€§æ•°æ®ï¼‰æ˜¯ç¬¦åˆæ­£æ€åˆ†å¸ƒçš„ï¼Œè¿™ä¸ªå‡è®¾å¯¹äºç°å®ç”Ÿæ´»ä¸­å¤§å¤šæ•°çš„è¿ç»­æ•°æ®ï¼ˆæ•°å€¼æ•°æ®ï¼‰éƒ½æ˜¯æˆç«‹çš„ï¼Œäº‹å®è¯æ˜ï¼Œå³ä½¿æœ‰çš„æ—¶å€™ä¸æˆç«‹ï¼Œå°±æ˜¯æ•°æ®ä¸æ˜¯æ­£æ€åˆ†å¸ƒï¼Œè¿›è¡Œæ ‡å‡†åŒ–ä¹Ÿæ˜¯æœ‰ç›Šçš„ã€‚

### æ•°æ®æ ‡å‡†åŒ–ä¸å½’ä¸€åŒ–ä»£ç 
```
# å½’ä¸€åŒ–å¹¶å±•ç¤ºéƒ¨åˆ†æ•°æ®
from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)
X_train_norm[:5,0:3]

# In[27]:
# æ ‡å‡†åŒ–å¹¶å±•ç¤ºéƒ¨åˆ†æ•°æ®
from sklearn.preprocessing import StandardScaler
stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)
X_test_std[:5,0:3]


# In[28]:
# æ ‡å‡†åŒ–çš„å®é™…è®¡ç®—æ–¹æ³•
ex = np.array([0, 1, 2, 3, 4])
print('standardized:', (ex - ex.mean()) / ex.std())

# In[29]:
# å½’ä¸€åŒ–çš„å®é™…è®¡ç®—æ–¹æ³• 
print('normalized:', (ex - ex.min()) / (ex.max() - ex.min()))

# In[30]:
# æ ‡å‡†åŒ–çš„å®é™…è®¡ç®—æ–¹æ³•
ex = np.array([10000, 20000, 30000, 40000, 50000])
print('standardized:', (ex - ex.mean()) / ex.std())

# In[31]:
# å½’ä¸€åŒ–çš„å®é™…è®¡ç®—æ–¹æ³• 
print('normalized:', (ex - ex.min()) / (ex.max() - ex.min()))
```

### æ ‡å‡†åŒ–åè®­ç»ƒæå‡ä¾‹å­
ä½¿ç”¨ä¸€ä¸ªå•å±‚ç¥ç»ç½‘ç»œï¼šè‡ªé€‚åº”çº¿æ€§ç¥ç»ç½‘ç»œï¼ˆAdaptive Linear Neuronï¼ŒAdalineï¼‰ä½œä¸ºä¸€ä¸ªåˆ†ç±»å™¨ã€‚

æ­¤å¤„åˆ†ç±»å™¨ä¸æ˜¯é‡ç‚¹ï¼Œå…ˆäº†è§£å³å¯ã€‚é‡ç‚¹æ˜¯æ ‡å‡†åŒ–å¯¹äºè®­ç»ƒçš„ä½œç”¨ã€‚

 Adalineä¸é€»è¾‘å›å½’çš„åŒºåˆ«åœ¨äºæ¿€åŠ±å‡½æ•°æ˜¯ä¸€ä¸ªy=xçš„çº¿æ€§å‡½æ•°ï¼Œé€»è¾‘å›å½’æ˜¯sigmoidæ¿€æ´»ã€‚
 
Adalineä¸çº¿æ€§å›å½’çš„åŒºåˆ«åœ¨äºæœ€åéœ€è¦è¿›è¡Œé‡åŒ–å’Œåˆ†ç±»ï¼Œä¸æ˜¯è¿ç»­å€¼å¾—å›å½’åˆ†æã€‚
![image](../../youdaonote-images/0B921F8F1B38474ABFB75F9E886BF04A.png)

**å›å¿†ï¼šä¸ºä¼˜åŒ–æ”¶æ•›æ•ˆæœï¼Œå¸¸å¸¸éœ€è¦é€šè¿‡å®éªŒæ¥æ‰¾åˆ°åˆé€‚çš„å­¦ä¹ é€Ÿç‡Î·ã€‚åˆ†åˆ«ä½¿ç”¨Î·ï¼0.1å’ŒÎ·ï¼0.0001ä¸¤ä¸ªå­¦ä¹ é€Ÿç‡æ¥ç»˜åˆ¶è¿­ä»£æ¬¡æ•°ä¸ä»£ä»·å‡½æ•°çš„å›¾åƒï¼Œä»¥è§‚å¯ŸAdalineé€šè¿‡è®­ç»ƒæ•°æ®è¿›è¡Œå­¦ä¹ çš„æ•ˆæœã€‚**

**æ³¨æ„ï¼šè¿™é‡Œçš„å­¦ä¹ é€Ÿç‡Î·å’Œè¿­ä»£æ¬¡æ•°n_iteréƒ½æ˜¯ç®—æ³•çš„è¶…å‚ï¼ˆhyperparameterï¼‰ã€‚åé¢è¿˜å°†å­¦ä¹ è‡ªåŠ¨è°ƒæ•´è¶…å‚å€¼ä»¥å¾—åˆ°åˆ†ç±»æ€§èƒ½æœ€ä¼˜æ¨¡å‹çš„å„ç§æŠ€æœ¯ã€‚**==è¶…å‚æ˜¯äººä¸ºè®¾ç½®çš„ï¼Œæ¯”å¦‚è¿™é‡Œçš„å­¦ä¹ ç‡å’Œè¿­ä»£æ¬¡æ•°ã€‚ä¸€èˆ¬è¯´çš„æ¨¡å‹çš„å‚æ•°ï¼Œæ˜¯è®­ç»ƒå‡ºæ¥çš„ï¼Œæ¯”å¦‚ç¥ç»ç½‘ç»œçš„æƒé‡ã€‚==

![image](../../youdaonote-images/20D4264DBE014E6F95B4FC937A395FF3.png)
1. ä»ä¸‹é¢ä»£ä»·å‡½æ•°è¾“å‡ºç»“æœçš„å›¾åƒä¸­å¯ä»¥çœ‹åˆ°ï¼Œé¢ä¸´ä¸¤ç§ä¸åŒç±»å‹çš„é—®é¢˜ã€‚å·¦è¾¹çš„å›¾
åƒæ˜¾ç¤ºäº†å­¦ä¹ é€Ÿç‡è¿‡å¤§å¯èƒ½ä¼šå‡ºç°çš„é—®é¢˜â€”â€”å¹¶æ²¡æœ‰ä½¿ä»£ä»·å‡½æ•°çš„å€¼å°½å¯èƒ½çš„ä½ï¼Œåè€Œå› ä¸ºç®—æ³•è·³è¿‡äº†å…¨å±€æœ€ä¼˜è§£ï¼Œå¯¼è‡´è¯¯å·®éšç€è¿­ä»£æ¬¡æ•°å¢åŠ è€Œå¢å¤§
2. è™½ç„¶åœ¨å³è¾¹çš„å›¾ä¸­ä»£ä»·å‡½æ•°é€æ¸å‡å°ï¼Œä½†æ˜¯é€‰æ‹©çš„å­¦ä¹ é€Ÿç‡Î·ï¼0.0001çš„å€¼å¤ªå°ï¼Œä»¥è‡´ä¸ºäº†è¾¾åˆ°ç®—æ³•æ”¶æ•›çš„ç›®æ ‡ï¼Œéœ€è¦æ›´å¤šçš„è¿­ä»£æ¬¡æ•°

**åŒæ ·0.01å­¦ä¹ ç‡ï¼Œæ•°æ®æ ‡å‡†åŒ–ä»¥å**
![image](../../youdaonote-images/0F0B2F81AD574498B2C8CDE86A66D760.png)

### å®ä¾‹ä»£ç å±•ç¤º
```
# ä¸€ä¸ªä¾‹å­å±•ç¤ºæ•°æ®æ ‡å‡†åŒ–åå¯¹äºè®­ç»ƒçš„æ”¹å–„ 
# In[]:
class AdalineGD(object):
    """ADAptive LInear NEuron classifier.

    Parameters
    ------------
    eta : float
      Learning rate (between 0.0 and 1.0)
    n_iter : int
      Passes over the training dataset.
    random_state : int
      Random number generator seed for random weight
      initialization.


    Attributes
    -----------
    w_ : 1d-array
      Weights after fitting.
    cost_ : list
      Sum-of-squares cost function value in each epoch.

    """
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        """ Fit training data.

        Parameters
        ----------
        X : {array-like}, shape = [n_samples, n_features]
          Training vectors, where n_samples is the number of samples and
          n_features is the number of features.
        y : array-like, shape = [n_samples]
          Target values.

        Returns
        -------
        self : object

        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])
        self.cost_ = []

        for i in range(self.n_iter):
            net_input = self.net_input(X)
            # Please note that the "activation" method has no effect
            # in the code since it is simply an identity function. We
            # could write `output = self.net_input(X)` directly instead.
            # The purpose of the activation is more conceptual, i.e.,  
            # in the case of logistic regression (as we will see later), 
            # we could change it to
            # a sigmoid function to implement a logistic regression classifier.
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            cost = (errors**2).sum() / 2.0
            self.cost_.append(cost)
        return self

    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def activation(self, X):
        """Compute linear activation"""
        return X

    def predict(self, X):
        """Return class label after unit step"""
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)

# è¯»å…¥æ•°æ®
# In[]:
import pandas as pd

data_dir = 'F:/2019-notebook/2017_2018_2/python_code/MTrain/MachineLearn/3_ML/5_Preprocess/'
df = pd.read_csv(data_dir + 'iris.data', header=None)
df.tail()

# In[]:


import matplotlib.pyplot as plt
import numpy as np

# select setosa and versicolor
y = df.iloc[0:100, 4].values  #df.iloc[0:100, 4] ä»dataframeä¸­é€‰æ‹©0-100è¡Œçš„ç¬¬4åˆ—ï¼Œå°±æ˜¯èŠ±çš„ç±»å‹å€¼
y

# In[]
# è®¾ç½®ä¸º-1å’Œ1ä¸¤ç±»ï¼ŒIris-setosaä¸º-1ï¼Œå…¶ä»–æ˜¯1ç±»
y = np.where(y == 'Iris-setosa', -1, 1)
y

# In[]
# extract sepal length and petal length
X = df.iloc[0:100, [0, 2]].values  #ç¬¬0åˆ—å’Œç¬¬2åˆ—å¯¹åº”èŠ±çš„ä¸¤ä¸ªç‰¹å¾å±æ€§
X
# In[]:
fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))

ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)
ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker='o')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('log(Sum-squared-error)')
ax[0].set_title('Adaline - Learning rate 0.01')

ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)
ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Sum-squared-error')
ax[1].set_title('Adaline - Learning rate 0.0001')

# plt.savefig('images/02_11.png', dpi=300)
plt.show()

# æ•°æ®æ ‡å‡†åŒ–ä»¥åæŒ‰ç…§å­¦ä¹ ç‡0.01å†è®­ç»ƒ

# In[]:
# standardize features
X_std = np.copy(X)
X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()
X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()


# In[]:
ada = AdalineGD(n_iter=15, eta=0.01)
ada.fit(X_std, y)

import matplotlib.pyplot as plt
plt.plot(range(1, len(ada.cost_) + 1), ada.cost_, marker='o')
plt.xlabel('Epochs')
plt.ylabel('Sum-squared-error')

plt.tight_layout()
plt.show()
```

## ç‰¹å¾é€‰æ‹©
- å¦‚æœä¸€ä¸ªæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®é›†ä¸Šçš„è¡¨ç°æ¯”åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šå¥½å¾ˆå¤šï¼Œè¿™æ„å‘³ç€æ¨¡å‹è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰äºè®­ç»ƒæ•°æ®ã€‚è¿‡æ‹Ÿåˆæ˜¯æŒ‡æ¨¡å‹å‚æ•°å¯¹äºè®­ç»ƒæ•°æ®é›†çš„ç‰¹å®šè§‚æµ‹å€¼æ‹Ÿåˆå¾—éå¸¸æ¥è¿‘ï¼Œä½†è®­ç»ƒæ•°æ®é›†çš„åˆ†å¸ƒä¸çœŸå®æ•°æ®å¹¶ä¸ä¸€è‡´â€”â€”æˆ‘ä»¬ç§°ä¹‹ä¸ºæ¨¡å‹å…·æœ‰è¾ƒé«˜çš„æ–¹å·®ã€‚
- äº§ç”Ÿè¿‡æ‹Ÿåˆçš„åŸå› æ˜¯å»ºç«‹åœ¨ç»™å®šè®­ç»ƒæ•°æ®é›†ä¸Šçš„æ¨¡å‹è¿‡äºå¤æ‚ï¼Œè€Œå¸¸ç”¨çš„é™ä½æ³›åŒ–è¯¯å·®çš„æ–¹æ¡ˆæœ‰ï¼š
    1. **æ”¶é›†æ›´å¤šçš„è®­ç»ƒæ•°æ®**
    2. **é€šè¿‡æ­£åˆ™åŒ–å¼•å…¥ç½šé¡¹**
    3. **é€‰æ‹©ä¸€ä¸ªå‚æ•°ç›¸å¯¹è¾ƒå°‘çš„ç®€å•æ¨¡å‹**
    4. **é™ä½æ•°æ®çš„ç»´åº¦**

### æ­£åˆ™åŒ–ç‰¹å¾é€‰æ‹©
![image](../../youdaonote-images/3E8C8523FB1B4EDFB96910258611D1F8.png)
- å¯¹äºL1æ­£åˆ™åŒ–ï¼Œåªæ˜¯ç®€å•åœ°å°†æƒé‡çš„å¹³æ–¹å’Œç”¨æƒé‡ç»å¯¹å€¼çš„å’Œæ¥æ›¿ä»£ã€‚
- ä¸L2æ­£åˆ™åŒ–ä¸åŒï¼ŒL1æ­£åˆ™åŒ–å¯ç”Ÿæˆç¨€ç–çš„ç‰¹å¾å‘é‡ï¼Œä¸”å¤§å¤šæ•°çš„æƒå€¼ä¸º0ã€‚
- å½“é«˜ç»´æ•°æ®é›†ä¸­åŒ…å«è®¸å¤šä¸ç›¸å…³çš„ç‰¹å¾ï¼Œå°¤å…¶æ˜¯ä¸ç›¸å…³çš„ç‰¹å¾æ•°é‡å¤§äºæ ·æœ¬æ•°é‡æ—¶ï¼Œæƒé‡çš„ç¨€ç–åŒ–å¤„ç†èƒ½å¤Ÿå‘æŒ¥å¾ˆå¤§çš„ä½œç”¨ã€‚
- ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼ŒL1æ­£åˆ™åŒ–å¯ä»¥è¢«è§†ä½œä¸€ç§ç‰¹å¾é€‰æ‹©æŠ€æœ¯ã€‚

### L1å’ŒL2æ­£åˆ™åŒ–ç‰¹å¾é€‰æ‹©ä»£ç 
```
#### ç‰¹å¾é€‰æ‹©

# ## Sparse solutions with L1-regularization
# ## é€šè¿‡L1æ­£åˆ™åŒ–çš„æ–¹æ³•ç¨€ç–æƒé‡ç­‰å‚æ•°ï¼Œä»è€Œè¾¾åˆ°ç‰¹å¾é€‰æ‹©ç›®çš„

# For regularized models in scikit-learn that support L1 regularization, 
# we can simply set the `penalty` parameter to `'l1'` to obtain a sparse solution:

# è¯»å…¥æ•°æ®
# In[]:
import numpy as np
import pandas as pd

data_dir = 'C:/video/æœºå™¨å­¦ä¹ /9 ï¼ˆå¿…ä¿®ï¼‰ç¬¬äº”éƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†/'
df_wine = pd.read_csv(data_dir+'wine.data', header=None)
df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']
df_wine.head()

# In[]
df_wine.shape

# In[]:

print('Class labels', np.unique(df_wine['Class label']))

# In[]:
from sklearn.model_selection import train_test_split

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

X_train, X_test, y_train, y_test =    train_test_split(X, y, 
                     test_size=0.3, 
                     random_state=0, 
                     stratify=y)

# In[]:
from sklearn.preprocessing import StandardScaler

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)


# In[]:
from sklearn.linear_model import LogisticRegression
# å¸¦æœ‰L1æƒ©ç½šçš„é€»è¾‘å›å½’å°±æ˜¯lassoå›å½’ï¼Œä½¿ç”¨çš„æ˜¯ç¼ºçœçš„æƒ©ç½šå€¼ï¼Œ C=1.0
LogisticRegression(penalty='l1')

# Applied to the standardized Wine data ...

# In[]:
from sklearn.linear_model import LogisticRegression
# c1è¶Šå° æ§åˆ¶åŠ›åº¦è¶Šå¤§
lr = LogisticRegression(penalty='l1', C=1.0, solver='liblinear')

lr.fit(X_train_std, y_train)
print('Training accuracy:', lr.score(X_train_std, y_train))
print('Test accuracy:', lr.score(X_test_std, y_test))

# In[]:
# å› ä¸ºè¾“å‡ºæœ‰ä¸‰ç±»ï¼Œå°±æ˜¯æœ€åä¸€å±‚æœ‰ä¸‰ä¸ªç¥ç»å…ƒï¼Œæ¯ä¸ªå¯¹åº”ä¸€ä¸ªåç½®é‡
# å°±æ˜¯wx+bä¸­çš„bå€¼
lr.intercept_


# In[]:
# floatè¾“å‡ºçš„ç²¾åº¦ï¼Œå³å°æ•°ç‚¹åç»´æ•°
np.set_printoptions(8)

# In[]:
# ä¸€å…±13ä¸ªç‰¹å¾ï¼Œå³13ä¸ªè¾“å…¥ç¥ç»å…ƒï¼Œè¿æ¥3ä¸ªè¾“å‡ºç¥ç»å…ƒï¼Œ
# æ‰€ä»¥æœ‰3ç»„æƒé‡ï¼Œæ¯ä¸€ç»„æ˜¯13ä¸ªæƒé‡å€¼
lr.coef_.shape

# In[]:
# 
lr.coef_


# In[]:
# æ­£åˆ™åŒ–ç³»æ•°å¯¹æƒé‡ç³»æ•°çš„å½±å“ï¼Œå¯ä»¥çœ‹å‡ºCè¶Šå°ï¼Œå½±å“è¶Šå¤§ï¼Œå¯ä»¥å¯¼è‡´æ‰€æœ‰ç‰¹å¾çš„æƒé‡ç³»æ•°ä¸º0
import matplotlib.pyplot as plt

fig = plt.figure()
ax = plt.subplot(111)
    
colors = ['blue', 'green', 'red', 'cyan', 
          'magenta', 'yellow', 'black', 
          'pink', 'lightgreen', 'lightblue', 
          'gray', 'indigo', 'orange']

weights, params = [], []
for c in np.arange(-4., 6.):
    lr = LogisticRegression(penalty='l1', C=10.**c, random_state=0, solver='liblinear')
    lr.fit(X_train_std, y_train)
    weights.append(lr.coef_[1])
    params.append(10**c)

weights = np.array(weights)

for column, color in zip(range(weights.shape[1]), colors):
    plt.plot(params, weights[:, column],
             label=df_wine.columns[column + 1],
             color=color)
plt.axhline(0, color='black', linestyle='--', linewidth=3)
plt.xlim([10**(-5), 10**5])
plt.ylabel('weight coefficient')
plt.xlabel('C')
plt.xscale('log')
plt.legend(loc='upper left')
ax.legend(loc='upper center', 
          bbox_to_anchor=(1.38, 1.03),
          ncol=1, fancybox=True)
plt.show()


### ç›´æ¥ä½¿ç”¨L1å’ŒL2æ­£åˆ™åŒ–çš„ç‰¹å¾é€‰æ‹©æ¨¡å‹
# In[]
# L1æ­£åˆ™ï¼Œlassoå›å½’ç‰¹å¾é€‰æ‹©
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler

# å¯ä»¥è®¾ç½®æƒ©ç½šç³»æ•°ï¼Œç¼ºçœæ˜¯1ï¼Œæ­¤å¤„ç³»æ•°è¶Šå¤§ï¼Œæƒ©ç½šåŠ›åº¦è¶Šå¤§
lasso = Lasso(0.2)
lasso.fit(X_train_std, y_train)
# æ˜¾ç¤ºç‰¹å¾çš„æƒé‡ç³»æ•°ï¼Œ0çš„ä½ç½®ï¼Œè¡¨ç¤ºç‰¹å¾å¯ä»¥å»é™¤
lasso.coef_

# In[]
# L2æ­£åˆ™ï¼Œridgeå›å½’ç‰¹å¾é€‰æ‹©
from sklearn.linear_model import Ridge
 
ridge = Ridge(0.2)
ridge.fit(X_train_std,y_train)
# ä¸L1æ­£åˆ™åŒ–å¯¹æ¯”å‘ç°ï¼Œ0çš„ä½ç½®å˜æˆäº†è¾ƒå°çš„å€¼
# æ‰€ä»¥å¯ä»¥è®¾ç½®ä¸€ä¸ªé˜ˆå€¼ï¼Œè¿‡æ»¤æ‰æƒé‡å°äºæŸä¸ªå€¼å¾—ç‰¹å¾
# å¯ä»¥çœ‹å‡ºL1æ­£åˆ™åŒ–åå®¹æ˜“å¾—åˆ°ä¸€ä¸ªç¨€ç–çŸ©é˜µï¼ŒL2æ­£åˆ™åŒ–åç³»æ•°ä¼šè¶‹äºå¹³å‡
ridge.coef_
```

### ç‰¹å¾é€‰æ‹©é™ç»´
- å¦å¤–ä¸€ç§é™ä½æ¨¡å‹å¤æ‚åº¦ä»è€Œè§£å†³è¿‡æ‹Ÿåˆé—®é¢˜çš„æ–¹æ³•æ˜¯é€šè¿‡ç‰¹å¾é€‰æ‹©è¿›è¡Œé™ç»´ï¼ˆdimensionality reductionï¼‰ï¼Œè¯¥æ–¹æ³•å¯¹æœªç»æ­£åˆ™åŒ–å¤„ç†çš„æ¨¡å‹ç‰¹åˆ«æœ‰æ•ˆã€‚

- é™ç»´æŠ€æœ¯ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªå¤§ç±»ï¼š==ç‰¹å¾é€‰æ‹©å’Œç‰¹å¾æå–==ã€‚é€šè¿‡ç‰¹å¾é€‰æ‹©ï¼Œå¯ä»¥é€‰å‡ºåŸå§‹ç‰¹å¾çš„==ä¸€ä¸ªå­é›†==ã€‚è€Œåœ¨ç‰¹å¾æå–ä¸­ï¼Œé€šè¿‡å¯¹ç°æœ‰çš„ç‰¹å¾ä¿¡æ¯è¿›è¡Œæ¨æ¼”ï¼Œæ„é€ å‡ºä¸€ä¸ª==æ–°çš„ç‰¹å¾å­ç©ºé—´==ã€‚åœ¨æœ¬èŠ‚ï¼Œå°†ç€çœ¼äºç‰¹å¾é€‰æ‹©ç®—æ³•ã€‚

#### åºåˆ—ç‰¹å¾é€‰æ‹©ç®—æ³•
1. åºåˆ—ç‰¹å¾é€‰æ‹©ç®—æ³•ç³»ä¸€ç§è´ªå©ªæœç´¢ç®—æ³•ï¼Œç”¨äºå°†åŸå§‹çš„dç»´ç‰¹å¾ç©ºé—´å‹ç¼©åˆ°ä¸€ä¸ªkç»´ç‰¹å¾å­ç©ºé—´ï¼Œå…¶ä¸­k<dã€‚
2. ä½¿ç”¨ç‰¹å¾é€‰æ‹©ç®—æ³•å‡ºäºä»¥ä¸‹è€ƒè™‘ï¼šèƒ½å¤Ÿå‰”é™¤ä¸ç›¸å…³ç‰¹å¾æˆ–å™ªå£°ï¼Œè‡ªåŠ¨é€‰å‡ºä¸é—®é¢˜æœ€ç›¸å…³çš„ç‰¹å¾å­é›†ï¼Œä»è€Œæé«˜è®¡ç®—æ•ˆç‡æˆ–æ˜¯é™ä½æ¨¡å‹çš„æ³›åŒ–è¯¯å·®ã€‚è¿™åœ¨æ¨¡å‹ä¸æ”¯æŒæ­£åˆ™åŒ–æ—¶å°¤ä¸ºæœ‰æ•ˆã€‚
3. ä¸€ä¸ªç»å…¸çš„åºåˆ—ç‰¹å¾é€‰æ‹©ç®—æ³•æ˜¯åºåˆ—åå‘é€‰æ‹©ç®—æ³•ï¼ˆSequential Backwar Selectionï¼ŒSBSï¼‰ï¼Œå…¶ç›®çš„æ˜¯åœ¨åˆ†ç±»æ€§èƒ½è¡°å‡æœ€å°çš„çº¦æŸä¸‹ï¼Œé™ä½åŸå§‹ç‰¹å¾ç©ºé—´ä¸Šçš„æ•°æ®ç»´åº¦ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚
4. åœ¨æŸäº›æƒ…å†µä¸‹ï¼ŒSBSç”šè‡³å¯ä»¥åœ¨æ¨¡å‹é¢ä¸´è¿‡æ‹Ÿåˆé—®é¢˜æ—¶æé«˜æ¨¡å‹çš„é¢„æµ‹èƒ½åŠ›ã€‚
5. SBSç®—æ³•èƒŒåçš„ç†å¿µéå¸¸ç®€å•ï¼šSBSä¾æ¬¡ä»ç‰¹å¾é›†åˆä¸­åˆ é™¤æŸäº›ç‰¹å¾ï¼Œç›´åˆ°æ–°çš„ç‰¹å¾å­ç©ºé—´åŒ…å«æŒ‡å®šæ•°é‡çš„ç‰¹å¾ã€‚ä¸ºäº†ç¡®å®šæ¯ä¸€æ­¥ä¸­æ‰€éœ€åˆ é™¤çš„ç‰¹å¾ï¼Œå®šä¹‰ä¸€ä¸ªéœ€è¦æœ€å°åŒ–çš„æ ‡å‡†è¡¡é‡å‡½æ•°Jã€‚
6. è¯¥å‡½æ•°çš„è®¡ç®—å‡†åˆ™æ˜¯ï¼šæ¯”è¾ƒåˆ¤å®šåˆ†ç±»å™¨çš„æ€§èƒ½åœ¨åˆ é™¤æŸä¸ªç‰¹å®šç‰¹å¾å‰åçš„å·®å¼‚ã€‚
7. ç”±æ­¤ï¼Œæ¯ä¸€æ­¥ä¸­å¾…åˆ é™¤çš„ç‰¹å¾ï¼Œå°±æ˜¯é‚£äº›èƒ½å¤Ÿä½¿å¾—æ ‡å‡†è¡¡é‡å‡½æ•°å€¼å°½å¯èƒ½å¤§çš„ç‰¹å¾ï¼Œæˆ–è€…æ›´ç›´è§‚åœ°è¯´ï¼šæ¯ä¸€æ­¥ä¸­ç‰¹å¾è¢«åˆ é™¤åï¼Œæ‰€å¼•èµ·çš„æ¨¡å‹æ€§èƒ½æŸå¤±æœ€å°ã€‚
8. é—æ†¾çš„æ˜¯ï¼Œscikit-learnä¸­å¹¶æ²¡æœ‰å®ç°SBSç®—æ³•ã€‚ä¸è¿‡å®ƒç›¸å½“ç®€å•ï¼Œå¯ä»¥ä½¿ç”¨Pythonæ¥å®ç°

**åœ¨SBSçš„å®ç°ä¸­ï¼Œå·²ç»åœ¨fitå‡½æ•°å†…å°†æ•°æ®é›†åˆ’åˆ†ä¸ºæµ‹è¯•æ•°æ®é›†å’Œè®­ç»ƒæ•°æ®é›†ï¼Œä¾æ—§å°†è®­ç»ƒæ•°æ®é›†X_trainè¾“å…¥åˆ°ç®—æ³•ä¸­ã€‚SBSç®—æ³•åœ¨æ¯ä¸€æ­¥ä¸­éƒ½å­˜å‚¨äº†æœ€ä¼˜ç‰¹å¾å­é›†çš„åˆ†å€¼ï¼Œä¸‹é¢ç»˜åˆ¶å‡ºKNNåˆ†ç±»å™¨çš„åˆ†ç±»å‡†ç¡®ç‡ï¼Œå‡†ç¡®ç‡æ•°å€¼æ˜¯åœ¨éªŒè¯æ•°æ®é›†ä¸Šè®¡ç®—å¾—å‡ºçš„ã€‚ä»£ç å¦‚ä¸‹ï¼š**

```
# ## åºåˆ—ç‰¹å¾é€‰æ‹©ç®—æ³•
# ## Sequential feature selection algorithms


# In[]:
from sklearn.base import clone
from itertools import combinations
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


class SBS():
    def __init__(self, estimator, k_features, scoring=accuracy_score,
                 test_size=0.25, random_state=1):
        self.scoring = scoring
        self.estimator = clone(estimator)
        self.k_features = k_features
        self.test_size = test_size
        self.random_state = random_state

    def fit(self, X, y):

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size,
                                                            random_state=self.random_state)

        # X_trainæ•°æ®shapeæ˜¯ä¸¤ç»´ï¼Œshape[0]æ˜¯è¡Œæ•°è¡¨ç¤ºæ ·æœ¬æ•°ï¼Œshape[1]æ˜¯åˆ—æ•°è¡¨ç¤ºç‰¹å¾æ•°
        dim = X_train.shape[1]
        # è½¬ä¸ºä¸€ä¸ªèŒƒå›´ï¼Œæ¯”å¦‚å¦‚æœdim=3,ç»“æœæ˜¯ä¸€ä¸ªå…ƒç»„ç±»å‹(0ï¼Œ1ï¼Œ2)
        # self.indices_ï¼Œæ˜¯pythonçš„ç±»çš„çŸ¥è¯†ï¼Œè½¬ä¸ºæˆå‘˜å˜é‡
        self.indices_ = tuple(range(dim))
        # è½¬ä¸ºlistæ ¼å¼ [0,1,2]
        self.subsets_ = [self.indices_]
        # ä»¥å‡†ç¡®ç‡ä½œä¸ºåº¦é‡çš„scoreï¼Œè®¡ç®—å‡ºä»¥indices_ä½œä¸ºé€‰æ‹©ç‰¹å¾ä¸‹ï¼Œè®­ç»ƒååœ¨æµ‹è¯•é›†ä¸Šå‡†ç¡®ç‡
        score = self._calc_score(X_train, y_train,
                                 X_test, y_test, self.indices_)
        # ä¿å­˜è¿™æ¬¡çš„å‡†ç¡®ç‡ï¼Œå°±æ˜¯è¯´ï¼Œåœ¨æ²¡æœ‰ç‰¹å¾å‡å°‘çš„æƒ…å†µä¸‹çš„å‡†ç¡®ç‡å…ˆä¿å­˜
        self.scores_ = [score]

        # dimæ˜¯æ‰€æœ‰ç‰¹å¾æ•°ï¼Œk_featuresæ˜¯æœ€å°‘è¦æµ‹è¯•ä¿ç•™çš„ç‰¹å¾æ•°ï¼Œä¸€èˆ¬æ˜¯1ï¼Œ
        # ä½†æ˜¯ä¹Ÿä¸ç»å¯¹ï¼Œæ¯”å¦‚è¯´2ä¹Ÿè¡Œï¼Œæ ¹æ®è‡ªå·±éœ€è¦
        while dim > self.k_features:
            scores = []
            subsets = []

            # combinationsæ˜¯æ•°å­¦ä¸­çš„ç»„åˆæ¦‚å¿µï¼Œ
            # å‡è®¾indices_=(0ï¼Œ1ï¼Œ2)ï¼Œr = 3-1=2,è¡¨ç¤º3ä¸­ä¸¤ä¸¤ç»„åˆï¼Œ
            # ç»“æœæ˜¯ä¸‰ä¸ª(0, 1) (0, 2) (1, 2)ï¼Œå°±æ˜¯è¯´å–æ‰€æœ‰ç‰¹å¾ä¸­çš„ä¸¤ä¸ªè¿›è¡Œæµ‹è¯•
            # åé¢dimä¼šé€’å‡ï¼Œè¿™æ ·å°±æ˜¯å»3ä¸ªä¸­çš„æ¯ä¸ªç‰¹å¾è¿›è¡Œæµ‹è¯•ï¼Œè®¡ç®—æµ‹è¯•é›†çš„å‡†ç¡®ç‡
            for p in combinations(self.indices_, r=dim - 1):
                score = self._calc_score(X_train, y_train,
                                         X_test, y_test, p)
                scores.append(score)
                subsets.append(p)

            # è·å–å‡†ç¡®ç‡æœ€å¤§çš„ç´¢å¼•ï¼Œ0è¡¨ç¤ºæ‰€æœ‰ç‰¹å¾é›†åˆï¼Œ
            best = np.argmax(scores)
            self.indices_ = subsets[best]
            self.subsets_.append(self.indices_)
            dim -= 1

            self.scores_.append(scores[best])
        self.k_score_ = self.scores_[-1]

        return self

    def transform(self, X):
        return X[:, self.indices_]

    def _calc_score(self, X_train, y_train, X_test, y_test, indices):
        self.estimator.fit(X_train[:, indices], y_train)
        y_pred = self.estimator.predict(X_test[:, indices])
        score = self.scoring(y_test, y_pred)
        return score


# In[]:
# read data
data_dir = 'C:/video/æœºå™¨å­¦ä¹ /9 ï¼ˆå¿…ä¿®ï¼‰ç¬¬äº”éƒ¨åˆ†ï¼šæ•°æ®é¢„å¤„ç†/'
df_wine = pd.read_csv(data_dir + 'wine.data', header=None)

df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                   'Alcalinity of ash', 'Magnesium', 'Total phenols',
                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                   'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                   'Proline']
df_wine.tail()

# In[]:
print('Class labels', np.unique(df_wine['Class label']))

# In[25]:
# å°†å·²çŸ¥çš„æ•°æ®åŒ–ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
from sklearn.model_selection import train_test_split

X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.3,
                                                    random_state=0,
                                                    stratify=y)
X.shape, X_train.shape, X_test.shape

# In[]:
from sklearn.preprocessing import StandardScaler

stdsc = StandardScaler()
X_train_std = stdsc.fit_transform(X_train)
X_test_std = stdsc.transform(X_test)

# In[]:

import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=3)

# selecting features
sbs = SBS(knn, k_features=1)
sbs.fit(X_train_std, y_train)

# plotting performance of feature subsets
k_feat = [len(k) for k in sbs.subsets_]

plt.plot(k_feat, sbs.scores_, marker='o')
plt.ylim([0.7, 1.02])
plt.ylabel('Accuracy')
plt.xlabel('Number of features')
plt.grid()
plt.tight_layout()
plt.show()

# In[]:
# æ³¨æ„ï¼Œç¬¬ä¸€ä¸ªè¡¨ç¤ºç‰¹å¾æœ€å¤šçš„æ•°æ®ï¼Œæœ€åä¸€ä¸ªè¡¨ç¤ºç‰¹å¾æœ€å°‘
sbs.subsets_

# In[]
# å› ä¸ºç¬¬ä¸€ä¸ªè¡¨ç¤ºç‰¹å¾æœ€å¤šçš„æ•°æ®ï¼Œæœ€åä¸€ä¸ªè¡¨ç¤ºç‰¹å¾æœ€å°‘
# ä»0å¼€å§‹ç´¢å¼•ï¼Œç´¢å¼•5è¡¨ç¤ºæœ‰8ä¸ªç‰¹å¾çš„æ—¶å€™
k8 = list(sbs.subsets_[5])
print(df_wine.columns[1:][k8])

# In[]:


knn.fit(X_train_std, y_train)
print('Training accuracy:', knn.score(X_train_std, y_train))
print('Test accuracy:', knn.score(X_test_std, y_test))

# In[]:

knn.fit(X_train_std[:, k8], y_train)
print('Training accuracy:', knn.score(X_train_std[:, k8], y_train))
print('Test accuracy:', knn.score(X_test_std[:, k8], y_test))
```
![image](../../youdaonote-images/66E31ACBC7F242168ECE69BE61AA8AAD.png)

> é€šè¿‡å›¾å¯ä»¥çœ‹åˆ°ï¼šå½“å¢åŠ äº†ç‰¹å¾çš„æ•°é‡åï¼ŒKNNåˆ†ç±»å™¨åœ¨éªŒè¯æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡æé«˜äº†ã€‚æ­¤å¤–ï¼Œå›¾ä¸­è¿˜æ˜¾ç¤ºï¼Œå½“kï¼{4ï¼Œ8ï¼Œ9ï¼Œ10ï¼Œ11ï¼Œ12}æ—¶ï¼Œç®—æ³•å¯ä»¥è¾¾åˆ°ç™¾åˆ†ä¹‹ç™¾çš„å‡†ç¡®ç‡ã€‚

**æˆ‘ä»¬çœ‹ä¸€ä¸‹æ˜¯å“ª8ä¸ªç‰¹å¾åœ¨éªŒè¯æ•°æ®é›†ä¸Šæœ‰å¦‚æ­¤å¥½çš„è¡¨ç°ï¼š**
![image](../../youdaonote-images/2CD5389AD2964B1C94462539073CA393.png)

éªŒè¯ä¸€ä¸‹KNNåˆ†ç±»å™¨åœ¨åŸå§‹æµ‹è¯•é›†ä¸Šçš„è¡¨ç°ï¼š
![image](../../youdaonote-images/630DB936B9AA4DDDA067975900D549E9.png)
åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œåœ¨è®­ç»ƒæ•°æ®é›†ä¸Šä½¿ç”¨äº†æ‰€æœ‰çš„ç‰¹å¾å¹¶å¾—åˆ°äº†å¤§çº¦ä¸º94.4%çš„å‡†ç¡®ç‡ã€‚
ä¸è¿‡ï¼Œåœ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ï¼ˆçº¦ä¸º96.3%ï¼‰ã€‚ç°åœ¨åœ¨é€‰å®šçš„8ä¸ªç‰¹å¾é›†ä¸Šçœ‹ä¸€ä¸‹KNNçš„æ€§èƒ½ï¼š
![image](../../youdaonote-images/DB22A61A684142569E4B3EDAD9499A5B.png)
**å½“ç‰¹å¾æ•°é‡å‡å°‘æ—¶å€™ï¼Œéƒ½æœ‰æ‰€æé«˜ã€‚**

## ç‰¹å¾æ’åº
### é€šè¿‡éšæœºæ£®æ—åˆ¤å®šç‰¹å¾çš„é‡è¦æ€§
- **éšæœºæ£®æ—æ˜¯å†³ç­–æ ‘çš„é›†æˆæ–¹æ³•**ï¼Œé€šè¿‡è¯¥ç®—æ³•å¯ä»¥å¯¹ç‰¹å¾è¿›è¡Œä¸€ä¸ªæ’åº
- å¦ä¸€ç§ä»æ•°æ®é›†ä¸­é€‰æ‹©ç›¸å…³ç‰¹å¾çš„æœ‰æ•ˆæ–¹æ³•æ˜¯ä½¿ç”¨éšæœºæ£®æ—ï¼Œå¯ä»¥é€šè¿‡æ£®æ—ä¸­æ‰€æœ‰å†³ç­–æ ‘å¾—åˆ°çš„å¹³å‡ä¸çº¯åº¦è¡°å‡æ¥åº¦é‡ç‰¹å¾çš„é‡è¦æ€§ï¼Œè€Œä¸å¿…è€ƒè™‘æ•°æ®æ˜¯å¦çº¿æ€§å¯åˆ†ã€‚
- æ›´åŠ æ–¹ä¾¿çš„æ˜¯ï¼šscikit-learnä¸­å®ç°çš„éšæœºæ£®æ—å·²ç»ä¸ºæˆ‘ä»¬æ”¶é›†å¥½äº†å…³äºç‰¹å¾é‡è¦ç¨‹åº¦çš„ä¿¡æ¯ï¼Œåœ¨æ‹Ÿåˆäº†RandomForestClassifieråï¼Œ
å¯ä»¥é€šè¿‡feature_importances_å¾—åˆ°è¿™äº›å†…å®¹ã€‚
- å¯åœ¨è‘¡è„é…’æ•°æ®é›†ä¸Šè®­ç»ƒ500æ£µæ ‘ï¼Œå¹¶ä¸”åˆ†åˆ«æ ¹æ®å…¶é‡è¦ç¨‹åº¦å¯¹13ä¸ªç‰¹å¾ç»™å‡ºé‡è¦æ€§ç­‰çº§ã€‚è¯·è®°ä½ï¼š==æ— éœ€å¯¹åŸºäºæ ‘çš„æ¨¡å‹åšæ ‡å‡†åŒ–æˆ–å½’ä¸€åŒ–å¤„ç†==ã€‚

### éšæœºæ£®æ—ç‰¹å¾é€‰æ‹©ä»£ç 
```
# # Assessing feature importance with Random Forests

# In[]:
from sklearn.ensemble import RandomForestClassifier

feat_labels = df_wine.columns[1:]

forest = RandomForestClassifier(n_estimators=500,
                                random_state=1)

forest.fit(X_train, y_train)
importances = forest.feature_importances_

# In[]:
# æ¯ä¸ªç‰¹å¾é‡è¦æ€§
importances

# In[]:
# é‡è¦æ€§ä»å¤§åˆ°å°æ’åºåçš„åŸæœ‰ç´¢å¼•å€¼
# æ¯”å¦‚è¿”å›æ•°ç»„çš„ç¬¬ä¸€ä¸ªå€¼12ï¼Œè¡¨ç¤ºåŸæ¥æ•°ç»„ä¸­ç´¢å¼•æ˜¯12çš„å€¼æœ€å¤§
indices = np.argsort(importances)[::-1]
indices

# In[]:
for f in range(X_train.shape[1]):
    print("%2d) %-*s %f" % (f + 1, 30,
                            feat_labels[indices[f]],
                            importances[indices[f]]))

plt.title('Feature Importance')
plt.bar(range(X_train.shape[1]),
        importances[indices],
        align='center')

plt.xticks(range(X_train.shape[1]),
           feat_labels[indices], rotation=90)
plt.xlim([-1, X_train.shape[1]])
plt.tight_layout()
plt.show()

# In[]:
# è®¾ç½®ä¸€ä¸ªé—¨é™å€¼æ¥é€‰æ‹©ç‰¹å¾
from sklearn.feature_selection import SelectFromModel
# threshold é€‰æ‹©å¤§äº0.1çš„ç‰¹å¾
sfm = SelectFromModel(forest, threshold=0.1, prefit=True)
X_selected = sfm.transform(X_train)

X_selected.shape

# Now, let's print the 3 features that met the threshold criterion for feature selection that we set earlier (note that this code snippet does not appear in the actual book but was added to this notebook later for illustrative purposes):

# In[]:
# æ˜¾ç¤ºé€‰æ‹©çš„5ä¸ªç‰¹å¾
for f in range(X_selected.shape[1]):
    print("%2d) %-*s %f" % (f + 1, 30,
                            feat_labels[indices[f]],
                            importances[indices[f]]))
```
