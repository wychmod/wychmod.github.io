# 动深-4-深度学习计算
[toc]
## 4.1 模型构造
我们在上一章的其他 节中也使用了Sequential类构造模型。这里我们介绍另外一种基于==tf.keras.Model==类的模型构造方法：它让模型构造更加灵活。
## 4.1.1 build model from block
==tf.keras.Model==类是==tf.keras==模块里提供的一个模型构造类，我们可以继承它来定义我们想要的模型。下面继承==tf.keras.Model==类构造本节开头提到的多层感知机。这里定义的MLP类重载了==tf.keras.Model==类的==__init__==函数和==call==函数。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。
```
import tensorflow as tf
import numpy as np
print(tf.__version__)

class MLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平
        self.dense1 = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(units=10)

    def call(self, inputs):         
        x = self.flatten(inputs)   
        x = self.dense1(x)    
        output = self.dense2(x)     
        return output

```

以上的MLP类中无须定义反向传播函数。系统将通过自动求梯度而自动生成反向传播所需的backward函数。

我们可以实例化MLP类得到模型变量net。下面的代码初始化net并传入输入数据X做一次前向计算。其中，net(X)将调用MLP类定义的call函数来完成前向计算。

```
X = tf.random.uniform((2,20))
net = MLP()
net(X)

<tf.Tensor: id=62, shape=(2, 10), dtype=float32, numpy=
array([[ 0.15637134,  0.14062534, -0.11187253, -0.13151687,  0.12066578,
         0.15376692,  0.03429577,  0.07023033, -0.12030508, -0.38496107],
       [-0.02877349,  0.1088542 , -0.20668823,  0.08241277,  0.06292161,
         0.25310248,  0.04884301,  0.27015388, -0.13183925, -0.23431192]],
      dtype=float32)>
```

### 4.1.2 Sequential
我们刚刚提到，tf.keras.Model类是一个通用的部件。事实上，Sequential类继承自tf.keras.Model类。当模型的前向计算为简单串联各个层的计算时，可以通过更加简单的方式定义模型。这正是Sequential类的目的：它提供add函数来逐一添加串联的Block子类实例，而模型的前向计算就是将这些实例按添加的顺序逐一计算。

我们用Sequential类来实现前面描述的MLP类，并使用随机初始化的模型做一次前向计算。
```
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation=tf.nn.relu),
    tf.keras.layers.Dense(10),
])

model(X)

<tf.Tensor: id=117, shape=(2, 10), dtype=float32, numpy=
array([[-0.42563885, -0.11981717,  0.0838763 ,  0.04553887,  0.09710997,
         0.16843301,  0.15290505, -0.00364013, -0.13743742, -0.36868355],
       [-0.37125233, -0.18243487,  0.24916942, -0.04006755,  0.06090571,
         0.05331742,  0.24555533, -0.03183865, -0.10122052, -0.11752242]],
      dtype=float32)>

```

### 4.1.3 build complex model
虽然Sequential类可以使模型构造更加简单，且不需要定义call函数，但直接继承tf.keras.Model类可以极大地拓展模型构造的灵活性。下面我们构造一个稍微复杂点的网络FancyMLP。在这个网络中，我们通过constant函数创建训练中不被迭代的参数，即常数参数。在前向计算中，除了使用创建的常数参数外，我们还使用tensor的函数和Python的控制流，并多次调用相同的层。
```
class FancyMLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.flatten = tf.keras.layers.Flatten()
        self.rand_weight = tf.constant(
            tf.random.uniform((20,20)))
        self.dense = tf.keras.layers.Dense(units=20, activation=tf.nn.relu)

    def call(self, inputs):         
        x = self.flatten(inputs)   
        x = tf.nn.relu(tf.matmul(x, self.rand_weight) + 1)
        x = self.dense(x)    
        while tf.norm(x) > 1:
            x /= 2
        if tf.norm(x) < 0.8:
            x *= 10
        return tf.reduce_sum(x)


net = FancyMLP()
net(X)


<tf.Tensor: id=220, shape=(), dtype=float32, numpy=24.381481>


```
在这个FancyMLP模型中，我们使用了常数权重rand_weight（注意它不是模型参数）、做了矩阵乘法操作（tf.matmul）并重复使用了相同的Dense层。下面我们来测试该模型的随机初始化和前向计算。

因为FancyMLP和Sequential类都是tf.keras.Model类的子类，所以我们可以嵌套调用它们。
```
class NestMLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.net = tf.keras.Sequential()
        self.net.add(tf.keras.layers.Flatten())
        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))
        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))
        self.dense = tf.keras.layers.Dense(units=16, activation=tf.nn.relu)


    def call(self, inputs):         
        return self.dense(self.net(inputs))

net = tf.keras.Sequential()
net.add(NestMLP())
net.add(tf.keras.layers.Dense(20))
net.add(FancyMLP())

net(X)

```
## 4.2 模型参数的访问、初始化和共享
本节将深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数。

我们先定义一个与上一节中相同的含单隐藏层的多层感知机。我们依然使用默认方式初始化它的参数，并做一次前向计算。
```
import tensorflow as tf
import numpy as np
print(tf.__version__)

net = tf.keras.models.Sequential()
net.add(tf.keras.layers.Flatten())
net.add(tf.keras.layers.Dense(256,activation=tf.nn.relu))
net.add(tf.keras.layers.Dense(10))

X = tf.random.uniform((2,20))
Y = net(X)
Y

<tf.Tensor: id=62, shape=(2, 10), dtype=float32, numpy=
array([[ 0.15294254,  0.0355227 ,  0.05113338,  0.06625789,  0.12223213,
        -0.5954561 ,  0.38035268, -0.17244355,  0.6725004 ,  0.00750941],
       [ 0.12288147, -0.2162356 , -0.02103446,  0.14871466,  0.10256162,
        -0.57710034,  0.22278625, -0.21283135,  0.52407515, -0.1426214 ]],
      dtype=float32)>
```
### 4.2.1 access model parameters
对于使用Sequential类构造的神经网络，我们可以通过weights属性来访问网络任一层的权重。回忆一下上一节中提到的Sequential类与tf.keras.Model类的继承关系。对于Sequential实例中含模型参数的层，我们可以通过tf.keras.Model类的weights属性来访问该层包含的所有参数。下面，访问多层感知机net中隐藏层的所有参数。索引0表示隐藏层为Sequential实例最先添加的层。
```
net.weights[0], type(net.weights[0])


(<tf.Variable 'sequential/dense/kernel:0' shape=(20, 256) dtype=float32, numpy=
 array([[-0.07852519, -0.03260126,  0.12601742, ...,  0.11949158,
          0.10042094, -0.10598273],
        [ 0.03567271, -0.11624913,  0.04699135, ..., -0.12115637,
          0.07733515,  0.13183317],
        [ 0.03837337, -0.11566538, -0.03314627, ..., -0.10877015,
          0.09273799, -0.07031895],
        ...,
        [-0.03430544, -0.00946991, -0.02949082, ..., -0.0956497 ,
         -0.13907745,  0.10703176],
        [ 0.00447187, -0.07251608,  0.08081181, ...,  0.02697623,
          0.05394638, -0.01623751],
        [-0.01946831, -0.00950103, -0.14190955, ..., -0.09374787,
          0.08714674,  0.12475103]], dtype=float32)>,
 tensorflow.python.ops.resource_variable_ops.ResourceVariable)
```

### 4.2.2 initialize params
我们在[“数值稳定性和模型初始化”]一节中描述了模型的默认初始化方法：权重参数元素为[-0.07, 0.07]之间均匀分布的随机数，偏差参数则全为0。但我们经常需要使用其他方法来初始化权重。在下面的例子中，我们将权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并依然将偏差参数清零。

```
class Linear(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.d1 = tf.keras.layers.Dense(
            units=10,
            activation=None,
            kernel_initializer=tf.random_normal_initializer(mean=0,stddev=0.01),
            bias_initializer=tf.zeros_initializer()
        )
        self.d2 = tf.keras.layers.Dense(
            units=1,
            activation=None,
            kernel_initializer=tf.ones_initializer(),
            bias_initializer=tf.ones_initializer()
        )

    def call(self, input):
        output = self.d1(input)
        output = self.d2(output)
        return output

net = Linear()
net(X)
net.get_weights()


[array([[-0.00306494,  0.01149799,  0.00900665, -0.00952527, -0.00651997,
      0.00010531,  0.00802666, -0.01102469,  0.01838934,  0.00915548],
    [ 0.00401672,  0.01788972, -0.00245794, -0.01051202,  0.02268461,
     -0.00271502, -0.00447782,  0.00636486,  0.00408998, -0.01373187],
    [-0.00468962, -0.00180526, -0.0117501 ,  0.01840584,  0.00044537,
     -0.00745311,  0.01155732, -0.00615015, -0.00942082, -0.00023081],
    [-0.01116156, -0.00614527, -0.00119119, -0.00843481,  0.01192368,
      0.00889105, -0.01000126, -0.0017869 , -0.00833272,  0.0019026 ],
    [ 0.0183291 , -0.00640716,  0.00936602,  0.01040828, -0.00140882,
     -0.00143817,  0.00126366,  0.01094474,  0.0132029 ,  0.00405393],
    [-0.00548183, -0.00489746, -0.01264372, -0.00501967,  0.00602909,
      0.00439432,  0.02449438,  0.00426046, -0.0017243 , -0.00319188],
    [-0.00034199, -0.00648715, -0.00694025, -0.00984227,  0.02798587,
     -0.01283635, -0.01735584, -0.00181439,  0.01585936,  0.00348289],
    [ 0.00181157, -0.00343991,  0.01415697, -0.00160312,  0.0018713 ,
     -0.00968461, -0.00268579,  0.01320006, -0.00041133, -0.01282531],
    [-0.0145638 ,  0.0096653 , -0.00787722, -0.00073892, -0.00222261,
      0.0031008 , -0.01858314,  0.00559973,  0.00439452, -0.02467434],
    [-0.00303086,  0.0015006 , -0.00920389,  0.01035136, -0.00040001,
     -0.00945453, -0.00506378,  0.00816534,  0.00347233,  0.01201165],
    [ 0.01979353,  0.00881971, -0.00060045, -0.00671935,  0.02482731,
     -0.0039808 ,  0.01195751, -0.00499541, -0.01421177,  0.00125722],
    [-0.00206965,  0.00737946,  0.02711954, -0.00566722, -0.01916223,
      0.00635906, -0.00112362,  0.00351852,  0.0027598 ,  0.00804986],
    [ 0.00190901,  0.00799948, -0.01007551, -0.00751526,  0.0027352 ,
     -0.00126002,  0.00079498, -0.00190032, -0.00912007,  0.00432031],
    [-0.00574654,  0.00703932,  0.00375365,  0.01700558, -0.00392553,
      0.00246399,  0.00686003, -0.00327425, -0.00158563,  0.01139532],
    [-0.010441  , -0.01566261,  0.01807244, -0.01265192, -0.00422926,
     -0.00729915, -0.00717674, -0.00036729,  0.00728995,  0.0034066 ],
    [-0.00497032, -0.01395558, -0.00276683,  0.0114197 , -0.01044411,
     -0.01518542,  0.00793149, -0.00169621, -0.008745  , -0.00825851],
    [-0.00098009, -0.00765272, -0.01993775,  0.0207908 , -0.0088134 ,
      0.01211826,  0.0033179 ,  0.0064116 ,  0.00399073,  0.00067746],
    [ 0.00282402,  0.00589997,  0.00674444, -0.01209166, -0.00875635,
      0.01789016, -0.00037993,  0.00392861,  0.02248183, -0.00427692],
    [-0.00629026, -0.01388059,  0.0160582 ,  0.00855581,  0.00170209,
      0.00430258,  0.0092911 ,  0.00232163,  0.00591121,  0.02038265],
    [-0.00792203, -0.00259904, -0.00109487, -0.00959524, -0.00030968,
     -0.01322429,  0.00489308,  0.00503101,  0.01801165,  0.00972504]],
   dtype=float32),
 array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),
 array([[1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.]], dtype=float32),
 array([1.], dtype=float32)]
```

### 4.2.3 define initializer
可以使用tf.keras.initializers类中的方法实现自定义初始化。
```
def my_init():
    return tf.keras.initializers.Ones()

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(64, kernel_initializer=my_init()))

Y = model(X)
model.weights[0]

<tf.Variable 'sequential_1/dense_4/kernel:0' shape=(20, 64) dtype=float32, numpy=
array([[1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       ...,
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.],
       [1., 1., 1., ..., 1., 1., 1.]], dtype=float32)>
```

## 4.3 延后初始化
到目前为止，我们忽略了建立网络时需要做的以下这些事情：

- 我们定义了网络架构，但没有指定输入维度。

- 我们添加层时没有指定前一层的输出维度。

- 我们在初始化参数时，甚至没有足够的信息来确定模型应该包含多少参数。

你可能会对我们的代码能运行感到惊讶。 毕竟，深度学习框架无法判断网络的输入维度是什么。 这里的诀窍是框架的*延后初始化*（defers initialization）， 即直到数据第一次通过模型传递时，框架才会动态地推断出每个层的大小。

在以后，当使用卷积神经网络时， 由于输入维度（即图像的分辨率）将影响每个后续层的维数， 有了该技术将更加方便。 现在我们在编写代码时无须知道维度是什么就可以设置参数， 这种能力可以大大简化定义和修改模型的任务。 接下来，我们将更深入地研究初始化机制。

### 4.3.1. 实例化网络
首先，让我们实例化一个多层感知机。此时，因为输入维数是未知的，所以网络不可能知道输入层权重的维数。 因此，框架尚未初始化任何参数，我们通过尝试访问以下参数进行确认。
```
import tensorflow as tf

net = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation=tf.nn.relu),
    tf.keras.layers.Dense(10),
])

[net.layers[i].get_weights() for i in range(len(net.layers))]

[[], []]


```
请注意，每个层对象都存在，但权重为空。 使用net.get_weights()将抛出一个错误，因为权重尚未初始化。

接下来让我们将数据通过网络，最终使框架初始化参数。
```
X = tf.random.uniform((2, 20))
net(X)
[w.shape for w in net.get_weights()]

[(20, 256), (256,), (256, 10), (10,)]
```

一旦我们知道输入维数是20，框架可以通过代入值20来识别第一层权重矩阵的形状。 识别出第一层的形状后，框架处理第二层，依此类推，直到所有形状都已知为止。 注意，在这种情况下，只有第一层需要延迟初始化，但是框架仍是按顺序初始化的。 等到知道了所有的参数形状，框架就可以初始化参数。

###  4.3.2. 小结
- 延后初始化使框架能够自动推断参数形状，使修改模型架构变得容易，避免了一些常见的错误。

- 我们可以通过模型传递数据，使框架最终初始化参数。

## 4.4 自定义层
深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层和后面章节中将要介绍的卷积层、池化层与循环层。虽然tf.keras提供了大量常用的层，但有时候我们依然希望自定义层。本节将介绍如何自定义一个层，从而可以被重复调用。
```
import tensorflow as tf
import numpy as np
print(tf.__version__)

X = tf.random.uniform((2,20))
```
### 4.4.1 custom layer without parameters
我们先介绍如何定义一个不含模型参数的自定义层。事实上，这和[“模型构造”]一节中介绍的使用tf.keras.Model类构造模型类似。下面的CenteredLayer类通过继承tf.keras.layers.Layer类自定义了一个将输入减掉均值后输出的层，并将层的计算定义在了call函数里。这个层里不含模型参数。
```
class CenteredLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()

    def call(self, inputs):
        return inputs - tf.reduce_mean(inputs)

layer = CenteredLayer()
layer(np.array([1,2,3,4,5]))


<tf.Tensor: id=11, shape=(5,), dtype=int32, numpy=array([-2, -1,  0,  1,  2])>
```
我们也可以用它来构造更复杂的模型。
```
net = tf.keras.models.Sequential()
net.add(tf.keras.layers.Flatten())
net.add(tf.keras.layers.Dense(20))
net.add(CenteredLayer())

Y = net(X)
Y

<tf.Tensor: id=42, shape=(2, 20), dtype=float32, numpy=
array([[-0.2791378 , -0.80257636, -0.8498672 , -0.8917849 , -0.43128002,
         0.2557137 , -0.51745236,  0.31894356,  0.03016172,  0.5299317 ,
        -0.094203  , -0.3885942 ,  0.6737736 ,  0.5981153 ,  0.30068082,
         0.42632163,  0.3067779 ,  0.07029241,  0.0343143 ,  0.41021633],
       [ 0.0257766 , -0.4703896 , -0.9074424 , -1.2818251 ,  0.17860745,
         0.11847494, -0.14939149,  0.20248316, -0.140678  ,  0.6033463 ,
         0.13899392, -0.08732668,  0.08497022,  0.8094018 ,  0.20579913,
         0.40613335,  0.2509889 ,  0.34718364, -0.6298219 ,  0.59436864]],
      dtype=float32)>

```

### 4.4.2 custom layer with parameters
我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。
```
class myDense(tf.keras.layers.Layer):
    def __init__(self, units):
        super().__init__()
        self.units = units

    def build(self, input_shape):     # 这里 input_shape 是第一次运行call()时参数inputs的形状
        self.w = self.add_weight(name='w',
            shape=[input_shape[-1], self.units], initializer=tf.random_normal_initializer())
        self.b = self.add_weight(name='b',
            shape=[self.units], initializer=tf.zeros_initializer())

    def call(self, inputs):
        y_pred = tf.matmul(inputs, self.w) + self.b
        return y_pred
        
dense = myDense(3)
dense(X)
dense.get_weights()

```
我们也可以使用自定义层构造模型。
```
net = tf.keras.models.Sequential()
net.add(myDense(8))
net.add(myDense(1))

net(X)

```
## 读取和存储
到目前为止，我们介绍了如何处理数据以及如何构建、训练和测试深度学习模型。然而在实际中，我们有时需要把训练好的模型部署到很多不同的设备。在这种情况下，我们可以把内存中训练好的模型参数存储在硬盘上供后续读取使用。
```
import tensorflow as tf
import numpy as np
print(tf.__version__)

import numpy as np

x = tf.ones(3)
x

np.save('x.npy', x)
x2 = np.load('x.npy')
x2

y = tf.zeros(4)
np.save('xy.npy',[x,y])
x2, y2 = np.load('xy.npy', allow_pickle=True)
(x2, y2)


mydict = {'x': x, 'y': y}
np.save('mydict.npy', mydict)
mydict2 = np.load('mydict.npy', allow_pickle=True)
mydict2

```
```
X = tf.random.normal((2,20))
X

class MLP(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平
        self.dense1 = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(units=10)

    def call(self, inputs):         
        x = self.flatten(inputs)   
        x = self.dense1(x)    
        output = self.dense2(x)     
        return output

net = MLP()
Y = net(X)
Y

net.save_weights("4.5saved_model.h5")

net2 = MLP()
net2(X)
net2.load_weights("4.5saved_model.h5")
Y2 = net2(X)
Y2 == Y

```

## 4.6 GPU计算
