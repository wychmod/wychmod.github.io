# 动深-7-优化算法

## 7.1 优化与深度学习
。在一个深度学习问题中，我们通常会预先定义一个损失函数。有了损失函数以后，我们就可以使用优化算法试图将其最小化。在优化中，这样的损失函数通常被称作优化问题的目标函数（objective function）。依据惯例，优化算法通常只考虑最小化目标函数。其实，任何最大化问题都可以很容易地转化为最小化问题，只需令目标函数的相反数为新的目标函数即可。

### 7.1.1 优化与深度学习的关系
我们区分了训练误差和泛化误差。 由于优化算法的目标函数通常是一个基于训练数据集的损失函数，优化的目标在于降低训练误差。 而深度学习的目标在于降低泛化误差。为了降低泛化误差，除了使用优化算法降低训练误差以外，还需要注意应对过拟合。

### 7.1.2 优化在深度学习中的挑战
深度学习中绝大多数目标函数都很复杂。因此，很多优化问题并不存在解析解，而需要使用基于数值方法的优化算法找到近似解，即数值解。本书中讨论的优化算法都是这类基于数值方法的算法。为了求得最小化目标函数的数值解，我们将通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。

```
%matplotlib inline
import sys
import tensorflow as tf
sys.path.append("..") 
import d2lzh_tensorflow2 as d2l
from mpl_toolkits import mplot3d # 三维画图
import numpy as np

```

#### 7.1.2.1 局部最小值
![image.png](https://note.youdao.com/yws/res/a/WEBRESOURCE87e72b7ce9d9f1e6a117febf53a9c3ba)
```
def f(x):
    return x * np.cos(np.pi * x)

d2l.set_figsize((4.5, 2.5))
x = np.arange(-1.0, 2.0, 0.1)
fig,  = d2l.plt.plot(x, f(x))
fig.axes.annotate('local minimum', xy=(-0.3, -0.25), xytext=(-0.77, -1.0),
                  arrowprops=dict(arrowstyle='->'))
fig.axes.annotate('global minimum', xy=(1.1, -0.95), xytext=(0.6, 0.8),
                  arrowprops=dict(arrowstyle='->'))
d2l.plt.xlabel('x')
d2l.plt.ylabel('f(x)');

```
![image.png](https://note.youdao.com/yws/res/d/WEBRESOURCE81f0c55d0adccda4fcc65e4a0dddaa9d)

#### 7.1.2.2 鞍点
![image.png](https://note.youdao.com/yws/res/b/WEBRESOURCE61e134b2e17125b1b61810c9131e376b)
```
x = np.arange(-2.0, 2.0, 0.1)
fig, = d2l.plt.plot(x, x**3)
fig.axes.annotate('saddle point', xy=(0, -0.2), xytext=(-0.52, -5.0),
                  arrowprops=dict(arrowstyle='->'))
d2l.plt.xlabel('x')
d2l.plt.ylabel('f(x)');

```
![image.png](https://note.youdao.com/yws/res/e/WEBRESOURCEe34e2ccde57b6c53c45ca46f1e5c992e)

```
x, y = np.mgrid[-1: 1: 31j, -1: 1: 31j]
z = x**2 - y**2

ax = d2l.plt.figure().add_subplot(111, projection='3d')
ax.plot_wireframe(x, y, z, **{'rstride': 2, 'cstride': 2})
ax.plot([0], [0], [0], 'rx')
ticks = [-1,  0, 1]
d2l.plt.xticks(ticks)
d2l.plt.yticks(ticks)
ax.set_zticks(ticks)
d2l.plt.xlabel('x')
d2l.plt.ylabel('y');
```
![image.png](https://note.youdao.com/yws/res/8/WEBRESOURCE61dff1cb2d4db2671eb55c6df4c06b68)

**在深度学习中，虽然找到目标函数的全局最优解很难，但这并非必要。**

### 小结
- 由于优化算法的目标函数通常是一个基于训练数据集的损失函数，优化的目标在于降低训练误差。
- 由于深度学习模型参数通常都是高维的，目标函数的鞍点通常比局部最小值更常见。

## 7.2 梯度下降和随机梯度下降
虽然梯度下降在深度学习中很少被直接使用，但理解梯度的意义以及沿着梯度反方向更新自变量可能降低目标函数值的原因是学习后续优化算法的基础。随后，我们将引出随机梯度下降（stochastic gradient descent）。

### 7.2.1 一维梯度下降
![image.png](https://note.youdao.com/yws/res/4/WEBRESOURCE214a7669b0ff74f6d7c4228ef9e9f3c4)
![image.png](https://note.youdao.com/yws/res/f/WEBRESOURCEd0140be81f79fcfd5dd2b33d7feba81f)
![image.png](https://note.youdao.com/yws/res/2/WEBRESOURCEf7e2a92a6fc975a14fb093284785c042)
```
def gd(eta):
    x = 10
    results = [x]
    for i in range(10):
        x -= eta * 2 * x  # f(x) = x * x的导数为f'(x) = 2 * x
        results.append(x)
    print('epoch 10, x:', x)
    return results

res = gd(0.2)

```

### 7.2.2 学习率
![image.png](https://note.youdao.com/yws/res/f/WEBRESOURCE9c6bf08ba1d6914c25a7a2cc484f067f)
![image.png](https://note.youdao.com/yws/res/5/WEBRESOURCE3fd2124fb9d2d9a3d4d2a88f8420c4a5)

### 7.2.3 多维梯度下降
https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/chapter07_optimization/7.2_gd-sgd

### 7.2.4 随机梯度下降
![image.png](https://note.youdao.com/yws/res/d/WEBRESOURCEb95c8a2e6bff1e7121a55ca55f88fc2d)

### 小结
- 使用适当的学习率，沿着梯度反方向更新自变量可能降低目标函数值。梯度下降重复这一更新过程直到得到满足要求的解。
- 学习率过大或过小都有问题。一个合适的学习率通常是需要通过多次实验找到的。
- 当训练数据集的样本较多时，梯度下降每次迭代的计算开销较大，因而随机梯度下降通常更受青睐。

## 7.3 小批量随机梯度下降
在每一次迭代中，梯度下降使用整个训练数据集来计算梯度，因此它有时也被称为批量梯度下降（batch gradient descent）。而随机梯度下降在每次迭代中只随机采样一个样本来计算梯度。正如我们在前几章中所看到的，我们还可以在每轮迭代中随机均匀采样多个样本来组成一个小批量，然后使用这个小批量来计算梯度。下面就来描述小批量随机梯度下降。

![image.png](https://note.youdao.com/yws/res/c/WEBRESOURCE277d1d2687c6318bb732aa04f11b293c)

![image.png](https://note.youdao.com/yws/res/2/WEBRESOURCEac7af3b141400faeb483055a6dff7292)

### 7.3.3 简洁实现
同样，我们也无须自己实现小批量随机梯度下降算法。tensorflow.keras.optimizers 模块提供了很多常用的优化算法比如SGD、Adam和RMSProp等。下面我们创建一个用于优化model 所有参数的优化器实例，并指定学习率为0.05的小批量随机梯度下降（SGD）为优化算法
```
from tensorflow.keras import optimizers
trainer = optimizers.SGD(learning_rate=0.05)
```

### 小结
- 小批量随机梯度每次随机均匀采样一个小批量的训练样本来计算梯度。
- 在实际中，（小批量）随机梯度下降的学习率可以在迭代过程中自我衰减。
- 通常，小批量随机梯度在每个迭代周期的耗时介于梯度下降和随机梯度下降的耗时之间。

## 7.4 动量法
目标函数有关自变量的梯度代表了目标函数在自变量当前位置下降最快的方向。因此，梯度下降也叫作最陡下降（steepest descent）。在每次迭代中，梯度下降根据自变量当前位置，沿着当前位置的梯度更新自变量。然而，如果自变量的迭代方向仅仅取决于自变量当前位置，这可能会带来一些问题。

### 7.4.1 梯度下降的问题
![image.png](https://note.youdao.com/yws/res/3/WEBRESOURCE51539dac052125a0554e14a9893c9f93)

![image.png](https://note.youdao.com/yws/res/3/WEBRESOURCE6088eef7fad79ed8e8b2cb13b4d0d313)
![image.png](https://note.youdao.com/yws/res/5/WEBRESOURCE99e133c7963cffefe7df49f4ce81e4e5)

### 7.4.2 动量法
看不懂，大概是动量会使斜率的移动更加平滑。
https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/chapter07_optimization/7.4_momentum

![image.png](https://note.youdao.com/yws/res/e/WEBRESOURCEa3f92a28622188ffa2c06b38af96f48e)

![image.png](https://note.youdao.com/yws/res/b/WEBRESOURCEe4d7e625aeed85a5da84232349abf79b)

### 7.4.4 简洁实现
在Tensorflow中，只需要通过参数momentum来指定动量超参数即可使用动量法。
```
from tensorflow.keras import optimizers
trainer = optimizers.SGD(learning_rate=0.004,momentum=0.9)
d2l.train_tensorflow2_ch7(trainer, {'lr': 0.004, 'momentum': 0.9},
                    features, labels)

```

### 小结
- 动量法使用了指数加权移动平均的思想。它将过去时间步的梯度做了加权平均，且权重按时间步指数衰减。
- 动量法使得相邻时间步的自变量更新在方向上更加一致。

## 7.5 AdaGrad算法
**AdaGrad算法，它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题。**

### 7.5.1 算法
![image.png](https://note.youdao.com/yws/res/6/WEBRESOURCEadc804c9370f8d13fe75432e271ef6a6)

### 7.5.2 特点
**当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。**

![image.png](https://note.youdao.com/yws/res/7/WEBRESOURCE3a112872bacadff61ad7e661ba885ca7)

### 7.5.4 简洁实现
通过名称为Adagrad的优化器方法，我们便可使用Tensorflow2提供的AdaGrad算法来训练模型。
```
from tensorflow.keras import optimizers
trainer = optimizers.Adagrad(learning_rate=0.01)
d2l.train_tensorflow2_ch7(trainer, {'lr': 0.01},
                    features, labels)
    
```

### 小结
- AdaGrad算法在迭代过程中不断调整学习率，并让目标函数自变量中每个元素都分别拥有自己的学习率。
- 使用AdaGrad算法时，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。

## 7.6 RMSProp算法
当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解。为了解决这一问题，RMSProp算法对AdaGrad算法做了一点小小的修改。

### 7.6.1 算法
rms可以理解成动量法和AdaGrad的结合吧。

### 7.6.3 简洁实现
通过名称为RMSprop的优化器方法，我们便可使用Tensorflow2中提供的RMSProp算法来训练模型。注意，超参数γγ通过alpha指定。
```
from tensorflow.keras import optimizers
trainer = optimizers.RMSprop(learning_rate=0.01,rho=0.9)
d2l.train_tensorflow2_ch7(trainer, {'lr': 0.01},
                    features, labels)

```

### 小结
RMSProp算法和AdaGrad算法的不同在于，RMSProp算法使用了小批量随机梯度按元素平方的指数加权移动平均来调整学习率。

## 7.7 AdaDelta算法
除了RMSProp算法以外，另一个常用优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有用解的问题做了改进 [1]。有意思的是，**AdaDelta算法没有学习率这一超参数。**

### 7.7.1 算法
跟rms有细节差别。

### 7.7.3 简洁实现
```
from tensorflow import keras
trainer = keras.optimizers.Adadelta(learning_rate=0.01,rho=0.9)
d2l.train_tensorflow2_ch7(trainer, {'rho': 0.9}, features, labels)

```

### 小结
- AdaDelta算法没有学习率超参数，它通过使用有关自变量更新量平方的指数加权移动平均的项来替代RMSProp算法中的学习率。

## 7.8 Adam算法
Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均 [1]。下面我们来介绍这个算法。

> Adam算法可以看做是RMSProp算法与动量法的结合。

### 7.8.3 简洁实现
```
from tensorflow import keras
trainer = keras.optimizers.Adam(learning_rate=0.01)
d2l.train_tensorflow2_ch7(trainer, {'learning_rate': 0.01}, features, labels)

```

### 小结
- Adam算法在RMSProp算法的基础上对小批量随机梯度也做了指数加权移动平均。
- Adam算法使用了偏差修正。