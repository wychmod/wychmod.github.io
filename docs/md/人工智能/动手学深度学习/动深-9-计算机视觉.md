# 动深-9-计算机视觉
[toc]
## 9.1 图像增广
大规模数据集是成功应用深度神经网络的前提。图像增广（image augmentation）技术通过对训练图像做一系列随机改变，来产生相似但又不同的训练样本，从而扩大训练数据集的规模。图像增广的另一种解释是，随机改变训练样本可以降低模型对某些属性的依赖，从而提高模型的泛化能力。例如，我们可以对图像进行不同方式的裁剪，使感兴趣的物体出现在不同位置，从而减轻模型对物体出现位置的依赖性。我们也可以调整亮度、色彩等因素来降低模型对色彩的敏感度。
```
import tensorflow as tf
import numpy as np
print(tf.__version__)

```
### 常用的图像增广方法
我们来读取一张形状为400×500400×500（高和宽分别为400像素和500像素）的图像作为实验的样例。
```
from matplotlib import pyplot as plt

img = plt.imread('img/cat1.jpg')
plt.imshow(img)

# 绘图函数show_images
def show_images(imgs, num_rows, num_cols, scale=2):
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)
    for i in range(num_rows):
        for j in range(num_cols):
            axes[i][j].imshow(imgs[i * num_cols + j])
            axes[i][j].axes.get_xaxis().set_visible(False)
            axes[i][j].axes.get_yaxis().set_visible(False)
    return axes

def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):
    Y = [aug(img) for _ in range(num_rows * num_cols)]
    show_images(Y, num_rows, num_cols, scale)

```
大部分图像增广方法都有一定的随机性。为了方便观察图像增广的效果，接下来我们定义一个辅助函数apply。这个函数对输入图像img多次运行图像增广方法aug并展示所有的结果。

### 翻转和裁剪
左右翻转图像通常不改变物体的类别。它是最早也是最广泛使用的一种图像增广方法。下面我们通过tf.image.random_flip_left_right来实现一半概率的图像左右翻转。
```
apply(img, tf.image.random_flip_left_right)
```
![image.png](https://note.youdao.com/yws/res/a/WEBRESOURCE6e12e1f19911f81f6608324bcdd4475a)

上下翻转不如左右翻转通用。但是至少对于样例图像，上下翻转不会造成识别障碍。下面我们创建tf.image.random_flip_up_down实例来实现一半概率的图像上下翻转。

```
apply(img, tf.image.random_flip_up_down)
```
![image.png](https://note.youdao.com/yws/res/5/WEBRESOURCEe5a8b41313b99e63bb742874c065c7a5)

我们还可以通过对图像随机裁剪来让物体以不同的比例出现在图像的不同位置，这同样能够降低模型对目标位置的敏感性。

在下面的代码里，我们每次随机裁剪出一块面积为原面积10%∼100%10%∼100%的区域，且该区域的宽和高之比随机取自0.5∼20.5∼2，然后再将该区域的宽和高分别缩放到200像素。若无特殊说明，本节中aa和bb之间的随机数指的是从区间[a,b][a,b]中随机均匀采样所得到的连续值。

```
aug=tf.image.random_crop
num_rows=2
num_cols=4
scale=1.5
crop_size=200

Y = [aug(img, (crop_size, crop_size, 3)) for _ in range(num_rows * num_cols)]
show_images(Y, num_rows, num_cols, scale)

array([[<matplotlib.axes._subplots.AxesSubplot object at 0x00000186F310AD48>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F30BDE48>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x000001862700F648>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F30F1DC8>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x00000186F3093AC8>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F2EEBCC8>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F2E78E08>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F4104FC8>]],
      dtype=object)

```

![image.png](https://note.youdao.com/yws/res/c/WEBRESOURCE763616434c0bdebaed5ed391af729acc)

### 变化颜色
另一类增广方法是变化颜色。我们可以从4个方面改变图像的颜色：亮度、对比度、饱和度和色调。在下面的例子里，我们将图像的亮度随机变化为原图亮度的50%50%（即1−0.51−0.5）∼150%∼150%（即1+0.51+0.5）。
```
aug=tf.image.random_brightness
num_rows=2
num_cols=4
scale=1.5
max_delta=0.5

Y = [aug(img, max_delta) for _ in range(num_rows * num_cols)]
show_images(Y, num_rows, num_cols, scale)

array([[<matplotlib.axes._subplots.AxesSubplot object at 0x00000186F3086848>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F2FE8EC8>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F2FEE4C8>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186282D36C8>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x00000186F2D39888>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F2D70A48>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F2DA9C08>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F2F20E88>]],
      dtype=object)

```
![image.png](https://note.youdao.com/yws/res/e/WEBRESOURCEef1b13c33d937ba0a29410d6d7dec49e)

类似地，我们也可以随机变化图像的色调。
```
aug=tf.image.random_hue
num_rows=2
num_cols=4
scale=1.5
max_delta=0.5

Y = [aug(img, max_delta) for _ in range(num_rows * num_cols)]
show_images(Y, num_rows, num_cols, scale)

array([[<matplotlib.axes._subplots.AxesSubplot object at 0x00000186F3013888>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F30386C8>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F4142308>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F4178508>],
       [<matplotlib.axes._subplots.AxesSubplot object at 0x00000186F41B16C8>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F41E9888>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F4220A08>,
        <matplotlib.axes._subplots.AxesSubplot object at 0x00000186F4259C88>]],
      dtype=object)

```
![image.png](https://note.youdao.com/yws/res/b/WEBRESOURCEdbd80d16df8f466cbb93950348e47cbb)

### 使用图像增广训练模型
https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/chapter09_computer-vision/9.1_image-augmentation

## 9.2 微调
假设我们想从图像中识别出不同种类的椅子，然后将购买链接推荐给用户。一种可能的方法是先找出100种常见的椅子，为每种椅子拍摄1,000张不同角度的图像，然后在收集到的图像数据集上训练一个分类模型。这个椅子数据集虽然可能比Fashion-MNIST数据集要庞大，但样本数仍然不及ImageNet数据集中样本数的十分之一。这可能会导致适用于ImageNet数据集的复杂模型在这个椅子数据集上过拟合。同时，因为数据量有限，最终训练得到的模型的精度也可能达不到实用的要求。

为了应对上述问题，一个显而易见的解决办法是收集更多的数据。然而，收集和标注数据会花费大量的时间和资金。例如，为了收集ImageNet数据集，研究人员花费了数百万美元的研究经费。虽然目前的数据采集成本已降低了不少，但其成本仍然不可忽略。

另外一种解决办法是应用迁移学习（transfer learning），将从源数据集学到的知识迁移到目标数据集上。例如，虽然ImageNet数据集的图像大多跟椅子无关，但在该数据集上训练的模型可以抽取较通用的图像特征，从而能够帮助识别边缘、纹理、形状和物体组成等。这些类似的特征对于识别椅子也可能同样有效。

微调由以下4步构成。
1. 在源数据集（如ImageNet数据集）上预训练一个神经网络模型，即源模型。
2. 创建一个新的神经网络模型，即目标模型。它复制了源模型上除了输出层外的所有模型设计及其参数。我们假设这些模型参数包含了源数据集上学习到的知识，且这些知识同样适用于目标数据集。我们还假设源模型的输出层跟源数据集的标签紧密相关，因此在目标模型中不予采用。
3. 为目标模型添加一个输出大小为目标数据集类别个数的输出层，并随机初始化该层的模型参数。
4. 在目标数据集（如椅子数据集）上训练目标模型。我们将从头训练输出层，而其余层的参数都是基于源模型的参数微调得到的。

![image.png](https://note.youdao.com/yws/res/a/WEBRESOURCE89b9fb9cc89b61b4383284a01a6bbfaa)

当目标数据集远小于源数据集时，微调有助于提升模型的泛化能力。

### 热狗识别
https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/chapter09_computer-vision/9.2_fine-tuning

### 小结
- 迁移学习将从源数据集学到的知识迁移到目标数据集上。微调是迁移学习的一种常用技术。
- 目标模型复制了源模型上除了输出层外的所有模型设计及其参数，并基于目标数据集微调这些参数。而目标模型的输出层需要从头训练。
- 一般来说，微调参数会使用较小的学习率，而从头训练输出层可以使用较大的学习率。

## 9.3 目标检测和边界框
在图像分类任务里，我们假设图像里只有一个主体目标，并关注如何识别该目标的类别。然而，很多时候图像里有多个我们感兴趣的目标，我们不仅想知道它们的类别，还想得到它们在图像中的具体位置。在计算机视觉里，我们将这类任务称为目标检测（object detection）或物体检测。

目标检测在多个领域中被广泛使用。例如，在无人驾驶里，我们需要通过识别拍摄到的视频图像里的车辆、行人、道路和障碍的位置来规划行进线路。机器人也常通过该任务来检测感兴趣的目标。安防领域则需要检测异常目标，如歹徒或者炸弹。

### 边界框
在目标检测里，我们通常使用边界框（bounding box）来描述目标位置。边界框是一个矩形框，可以由矩形左上角的xx和yy轴坐标与右下角的xx和yy轴坐标确定。我们根据上面的图的坐标信息来定义图中狗和猫的边界框。图中的坐标原点在图像的左上角，原点往右和往下分别为xx轴和yy轴的正方向。
### 小结
- 在目标检测里不仅需要找出图像里面所有感兴趣的目标，而且要知道它们的位置。位置一般由矩形边界框来表示。

## 9.4 锚框
目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边缘从而更准确地预测目标的真实边界框（ground-truth bounding box）。不同的模型使用的区域采样方法可能不同。这里我们介绍其中的一种方法：它以每个像素为中心生成多个大小和宽高比（aspect ratio）不同的边界框。这些边界框被称为锚框（anchor box）。我们将在“单发多框检测（SSD）”一节基于锚框实践目标检测。

### 9.4.1. 生成多个锚框
https://trickygo.github.io/Dive-into-DL-TensorFlow2.0/#/chapter09_computer-vision/9.4_anchor

# 10-自然语言处理
