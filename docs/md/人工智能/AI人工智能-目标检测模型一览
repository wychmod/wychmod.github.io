<?xml version="1.0" encoding="UTF-8" standalone="no"?><note xmlns="http://note.youdao.com" file-version="0" schema-version="1.0.3"><head><list id="85-1639812165176" type="unordered"/><list id="67-1639812165221" type="unordered"/></head><body><para><coId>00-1639812165134</coId><text>目标检测是人工智能的一个重要应用，就是在图片中要将里面的物体识别出来，并标出物体的位置，一般需要经过两个步骤：</text><inline-styles><font-size><from>0</from><to>55</to><value>16</value></font-size><color><from>0</from><to>55</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>15-1639812165254</coId><text/><inline-styles/><styles/></para><para><coId>24-1639812165136</coId><text>1、分类，识别物体是什么</text><inline-styles><font-size><from>0</from><to>12</to><value>16</value></font-size><color><from>0</from><to>12</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>26-1639812165137</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>89-1639812165138</coId><source>https://upload-images.jianshu.io/upload_images/1409498-257c337a1893685d.png?imageMogr2/auto-orient/strip|imageView2/2/w/321/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>89-1639812165138</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>23-1639812165254</coId><text/><inline-styles/><styles/></para><para><coId>50-1639812165139</coId><text>2、定位，找出物体在哪里</text><inline-styles><font-size><from>0</from><to>12</to><value>16</value></font-size><color><from>0</from><to>12</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>16-1639812165140</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>36-1639812165141</coId><source>https://upload-images.jianshu.io/upload_images/1409498-95136d889d22133c.png?imageMogr2/auto-orient/strip|imageView2/2/w/369/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>70-1639812165141</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>40-1639812165255</coId><text/><inline-styles/><styles/></para><para><coId>06-1639812165142</coId><text>除了对单个物体进行检测，还要能支持对多个物体进行检测，如下图所示：</text><inline-styles><font-size><from>0</from><to>33</to><value>16</value></font-size><color><from>0</from><to>33</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>55-1639812165144</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>72-1639812165145</coId><source>https://upload-images.jianshu.io/upload_images/1409498-088a55d531e592b1.png?imageMogr2/auto-orient/strip|imageView2/2/w/374/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>28-1639812165145</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>35-1639812165255</coId><text/><inline-styles/><styles/></para><para><coId>22-1639812165146</coId><text>这个问题并不是那么容易解决，由于物体的尺寸变化范围很大、摆放角度多变、姿态不定，而且物体有很多种类别，可以在图片中出现多种物体、出现在任意位置。因此，目标检测是一个比较复杂的问题。</text><inline-styles><font-size><from>0</from><to>90</to><value>16</value></font-size><color><from>0</from><to>90</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>65-1639812165255</coId><text/><inline-styles/><styles/></para><para><coId>65-1639812165148</coId><text>最直接的方法便是构建一个深度神经网络，将图像和标注位置作为样本输入，然后经过CNN网络，再通过一个分类头（Classification head）的全连接层识别是什么物体，通过一个回归头（Regression head）的全连接层回归计算位置，如下图所示：</text><inline-styles><font-size><from>0</from><to>128</to><value>16</value></font-size><color><from>0</from><to>128</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>88-1639812165150</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>24-1639812165151</coId><source>https://upload-images.jianshu.io/upload_images/1409498-83d2e2fdccbfec2e.png?imageMogr2/auto-orient/strip|imageView2/2/w/558/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>72-1639812165152</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>40-1639812165255</coId><text/><inline-styles/><styles/></para><para><coId>32-1639812165152</coId><text>但“回归”不好做，计算量太大、收敛时间太长，应该想办法转为“分类”，这时容易想到套框的思路，即取不同大小的“框”，让框出现在不同的位置，计算出这个框的得分，然后取得分最高的那个框作为预测结果，如下图所示：</text><inline-styles><font-size><from>0</from><to>102</to><value>16</value></font-size><color><from>0</from><to>102</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>90-1639812165154</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>09-1639812165155</coId><source>https://upload-images.jianshu.io/upload_images/1409498-6efa7e028e8cedc8.png?imageMogr2/auto-orient/strip|imageView2/2/w/444/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>45-1639812165156</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>58-1639812165256</coId><text/><inline-styles/><styles/></para><para><coId>94-1639812165156</coId><text>根据上面比较出来的得分高低，选择了右下角的黑框作为目标位置的预测。</text><inline-styles><font-size><from>0</from><to>33</to><value>16</value></font-size><color><from>0</from><to>33</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>92-1639812165256</coId><text/><inline-styles/><styles/></para><para><coId>88-1639812165158</coId><text>但问题是：框要取多大才合适？太小，物体识别不完整；太大，识别结果多了很多其它信息。那怎么办？那就各种大小的框都取来计算吧。</text><inline-styles><font-size><from>0</from><to>61</to><value>16</value></font-size><color><from>0</from><to>61</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>99-1639812165256</coId><text/><inline-styles/><styles/></para><para><coId>81-1639812165159</coId><text>如下图所示（要识别一只熊），用各种大小的框在图片中进行反复截取，输入到CNN中识别计算得分，最终确定出目标类别和位置。</text><inline-styles><font-size><from>0</from><to>59</to><value>16</value></font-size><color><from>0</from><to>59</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>48-1639812165161</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>08-1639812165161</coId><source>https://upload-images.jianshu.io/upload_images/1409498-81a18966f58da445.png?imageMogr2/auto-orient/strip|imageView2/2/w/578/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>77-1639812165162</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>61-1639812165256</coId><text/><inline-styles/><styles/></para><para><coId>38-1639812165162</coId><text>这种方法效率很低，实在太耗时了。那有没有高效的目标检测方法呢？</text><inline-styles><font-size><from>0</from><to>31</to><value>16</value></font-size><color><from>0</from><to>31</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>67-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>25-1639812165163</coId><text>一、R-CNN 横空出世</text><inline-styles><font-size><from>0</from><to>12</to><value>16</value></font-size><color><from>0</from><to>12</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>30-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>52-1639812165164</coId><text>R-CNN（Region CNN，区域卷积神经网络）可以说是利用深度学习进行目标检测的开山之作，作者Ross Girshick多次在PASCAL VOC的目标检测竞赛中折桂，2010年更是带领团队获得了终身成就奖，如今就职于Facebook的人工智能实验室（FAIR）。</text><inline-styles><font-size><from>0</from><to>135</to><value>16</value></font-size><color><from>0</from><to>135</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>12-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>19-1639812165166</coId><text>R-CNN算法的流程如下</text><inline-styles><font-size><from>0</from><to>12</to><value>16</value></font-size><color><from>0</from><to>12</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>84-1639812165167</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>40-1639812165168</coId><source>https://upload-images.jianshu.io/upload_images/1409498-2af2b82b7ad49e3d.png?imageMogr2/auto-orient/strip|imageView2/2/w/737/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>44-1639812165168</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>02-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>87-1639812165169</coId><text>1、输入图像</text><inline-styles><font-size><from>0</from><to>6</to><value>16</value></font-size><color><from>0</from><to>6</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>42-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>37-1639812165169</coId><text>2、每张图像生成1K~2K个候选区域</text><inline-styles><font-size><from>0</from><to>18</to><value>16</value></font-size><color><from>0</from><to>18</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>07-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>11-1639812165170</coId><text>3、对每个候选区域，使用深度网络提取特征（AlextNet、VGG等CNN都可以）</text><inline-styles><font-size><from>0</from><to>41</to><value>16</value></font-size><color><from>0</from><to>41</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>95-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>77-1639812165170</coId><text>4、将特征送入每一类的SVM 分类器，判别是否属于该类</text><inline-styles><font-size><from>0</from><to>27</to><value>16</value></font-size><color><from>0</from><to>27</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>88-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>44-1639812165172</coId><text>5、使用回归器精细修正候选框位置</text><inline-styles><font-size><from>0</from><to>16</to><value>16</value></font-size><color><from>0</from><to>16</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>40-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>59-1639812165172</coId><text>下面展开进行介绍</text><inline-styles><font-size><from>0</from><to>8</to><value>16</value></font-size><color><from>0</from><to>8</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>99-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>30-1639812165173</coId><text>1、生成候选区域</text><inline-styles><font-size><from>0</from><to>8</to><value>16</value></font-size><color><from>0</from><to>8</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>19-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>43-1639812165173</coId><text>使用Selective Search（选择性搜索）方法对一张图像生成约2000-3000个候选区域，基本思路如下：</text><inline-styles><font-size><from>0</from><to>57</to><value>16</value></font-size><color><from>0</from><to>57</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>28-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>60-1639812165174</coId><text>（1）使用一种过分割手段，将图像分割成小区域</text><inline-styles><font-size><from>0</from><to>22</to><value>16</value></font-size><color><from>0</from><to>22</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>98-1639812165257</coId><text/><inline-styles/><styles/></para><para><coId>48-1639812165175</coId><text>（2）查看现有小区域，合并可能性最高的两个区域，重复直到整张图像合并成一个区域位置。优先合并以下区域：</text><inline-styles><font-size><from>0</from><to>51</to><value>16</value></font-size><color><from>0</from><to>51</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><list-item level="1" list-id="85-1639812165176"><coId>78-1639812165176</coId><text>颜色（颜色直方图）相近的</text><inline-styles><font-size><from>0</from><to>12</to><value>16</value></font-size><color><from>0</from><to>12</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height></styles></list-item><list-item level="1" list-id="85-1639812165176"><coId>47-1639812165177</coId><text>纹理（梯度直方图）相近的</text><inline-styles><font-size><from>0</from><to>12</to><value>16</value></font-size><color><from>0</from><to>12</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height></styles></list-item><list-item level="1" list-id="85-1639812165176"><coId>85-1639812165177</coId><text>合并后总面积小的</text><inline-styles><font-size><from>0</from><to>8</to><value>16</value></font-size><color><from>0</from><to>8</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height></styles></list-item><list-item level="1" list-id="85-1639812165176"><coId>01-1639812165178</coId><text>合并后，总面积在其BBOX中所占比例大的</text><inline-styles><font-size><from>0</from><to>20</to><value>16</value></font-size><color><from>0</from><to>20</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height></styles></list-item><para><coId>56-1639812165178</coId><text>在合并时须保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其它小区域，保证合并后形状规则。</text><inline-styles><font-size><from>0</from><to>48</to><value>16</value></font-size><color><from>0</from><to>48</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><font-size>16</font-size></styles></para><para><coId>24-1639812165179</coId><text>（3）输出所有曾经存在过的区域，即所谓候选区域</text><inline-styles><font-size><from>0</from><to>23</to><value>16</value></font-size><color><from>0</from><to>23</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><font-size>16</font-size></styles></para><para><coId>52-1639812165180</coId><text>2、特征提取</text><inline-styles><font-size><from>0</from><to>6</to><value>16</value></font-size><color><from>0</from><to>6</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><font-size>16</font-size></styles></para><para><coId>29-1639812165180</coId><text>使用深度网络提取特征之前，首先把候选区域归一化成同一尺寸227×227。</text><inline-styles><font-size><from>0</from><to>36</to><value>16</value></font-size><color><from>0</from><to>36</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><font-size>16</font-size></styles></para><para><coId>83-1639812165181</coId><text>使用CNN模型进行训练，例如AlexNet，一般会略作简化，如下图：</text><inline-styles><font-size><from>0</from><to>34</to><value>16</value></font-size><color><from>0</from><to>34</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><font-size>16</font-size></styles></para><image><coId>41-1639812165182</coId><source>https://upload-images.jianshu.io/upload_images/1409498-c779f38b40fec37d.png?imageMogr2/auto-orient/strip|imageView2/2/w/663/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>76-1639812165182</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>2.3076923076923075</line-height><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>47-1639812165183</coId><text>3、类别判断</text><inline-styles><font-size><from>0</from><to>6</to><value>16</value></font-size><color><from>0</from><to>6</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>88-1639812165259</coId><text/><inline-styles/><styles/></para><para><coId>85-1639812165183</coId><text>对每一类目标，使用一个线性SVM二类分类器进行判别。输入为深度网络（如上图的AlexNet）输出的4096维特征，输出是否属于此类。</text><inline-styles><font-size><from>0</from><to>66</to><value>16</value></font-size><color><from>0</from><to>66</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>11-1639812165259</coId><text/><inline-styles/><styles/></para><para><coId>18-1639812165184</coId><text>4、位置精修</text><inline-styles><font-size><from>0</from><to>6</to><value>16</value></font-size><color><from>0</from><to>6</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>35-1639812165259</coId><text/><inline-styles/><styles/></para><para><coId>59-1639812165185</coId><text>目标检测的衡量标准是重叠面积：许多看似准确的检测结果，往往因为候选框不够准确，重叠面积很小，故需要一个位置精修步骤，对于每一个类，训练一个线性回归模型去判定这个框是否框得完美，如下图：</text><inline-styles><font-size><from>0</from><to>92</to><value>16</value></font-size><color><from>0</from><to>92</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>21-1639812165186</coId><source>https://upload-images.jianshu.io/upload_images/1409498-5076733e2a7bb1ad.png?imageMogr2/auto-orient/strip|imageView2/2/w/559/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>48-1639812165187</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>63-1639812165259</coId><text/><inline-styles/><styles/></para><para><coId>29-1639812165188</coId><text>R-CNN将深度学习引入检测领域后，一举将PASCAL VOC上的检测率从35.1%提升到53.7%。</text><inline-styles><font-size><from>0</from><to>51</to><value>16</value></font-size><color><from>0</from><to>51</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>99-1639812165259</coId><text/><inline-styles/><styles/></para><para><coId>36-1639812165189</coId><text>二、Fast R-CNN大幅提速</text><inline-styles><font-size><from>0</from><to>16</to><value>16</value></font-size><color><from>0</from><to>16</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>95-1639812165259</coId><text/><inline-styles/><styles/></para><para><coId>56-1639812165189</coId><text>继2014年的R-CNN推出之后，Ross Girshick在2015年推出Fast R-CNN，构思精巧，流程更为紧凑，大幅提升了目标检测的速度。</text><inline-styles><font-size><from>0</from><to>74</to><value>16</value></font-size><color><from>0</from><to>74</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>37-1639812165259</coId><text/><inline-styles/><styles/></para><para><coId>84-1639812165190</coId><text>Fast R-CNN和R-CNN相比，训练时间从84小时减少到9.5小时，测试时间从47秒减少到0.32秒，并且在PASCAL VOC 2007上测试的准确率相差无几，约在66%-67%之间。</text><inline-styles><font-size><from>0</from><to>96</to><value>16</value></font-size><color><from>0</from><to>96</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>63-1639812165192</coId><source>https://upload-images.jianshu.io/upload_images/1409498-ca3bc602653d2f95.png?imageMogr2/auto-orient/strip|imageView2/2/w/522/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>76-1639812165192</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>61-1639812165259</coId><text/><inline-styles/><styles/></para><para><coId>57-1639812165193</coId><text>Fast R-CNN主要解决R-CNN的以下问题：</text><inline-styles><font-size><from>0</from><to>25</to><value>16</value></font-size><color><from>0</from><to>25</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>51-1639812165259</coId><text/><inline-styles/><styles/></para><para><coId>30-1639812165194</coId><text>1、训练、测试时速度慢</text><inline-styles><font-size><from>0</from><to>11</to><value>16</value></font-size><color><from>0</from><to>11</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>27-1639812165259</coId><text/><inline-styles/><styles/></para><para><coId>86-1639812165195</coId><text>R-CNN的一张图像内候选框之间存在大量重叠，提取特征操作冗余。而Fast R-CNN将整张图像归一化后直接送入深度网络，紧接着送入从这幅图像上提取出的候选区域。这些候选区域的前几层特征不需要再重复计算。</text><inline-styles><font-size><from>0</from><to>102</to><value>16</value></font-size><color><from>0</from><to>102</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>40-1639812165260</coId><text/><inline-styles/><styles/></para><para><coId>71-1639812165197</coId><text>2、训练所需空间大</text><inline-styles><font-size><from>0</from><to>9</to><value>16</value></font-size><color><from>0</from><to>9</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>63-1639812165260</coId><text/><inline-styles/><styles/></para><para><coId>05-1639812165197</coId><text>R-CNN中独立的分类器和回归器需要大量特征作为训练样本。Fast R-CNN把类别判断和位置精调统一用深度网络实现，不再需要额外存储。</text><inline-styles><font-size><from>0</from><to>68</to><value>16</value></font-size><color><from>0</from><to>68</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>82-1639812165260</coId><text/><inline-styles/><styles/></para><para><coId>35-1639812165198</coId><text>下面进行详细介绍</text><inline-styles><font-size><from>0</from><to>8</to><value>16</value></font-size><color><from>0</from><to>8</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>58-1639812165260</coId><text/><inline-styles/><styles/></para><para><coId>54-1639812165199</coId><text>1、在特征提取阶段，通过CNN（如AlexNet）中的conv、pooling、relu等操作都不需要固定大小尺寸的输入，因此，在原始图片上执行这些操作后，输入图片尺寸不同将会导致得到的feature map（特征图）尺寸也不同，这样就不能直接接到一个全连接层进行分类。</text><inline-styles><font-size><from>0</from><to>135</to><value>16</value></font-size><color><from>0</from><to>135</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>13-1639812165260</coId><text/><inline-styles/><styles/></para><para><coId>06-1639812165201</coId><text>在Fast R-CNN中，作者提出了一个叫做ROI Pooling的网络层，这个网络层可以把不同大小的输入映射到一个固定尺度的特征向量。ROI Pooling层将每个候选区域均匀分成M×N块，对每块进行max pooling。将特征图上大小不一的候选区域转变为大小统一的数据，送入下一层。这样虽然输入的图片尺寸不同，得到的feature map（特征图）尺寸也不同，但是可以加入这个神奇的ROI Pooling层，对每个region都提取一个固定维度的特征表示，就可再通过正常的softmax进行类型识别。</text><inline-styles><font-size><from>0</from><to>253</to><value>16</value></font-size><color><from>0</from><to>253</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>47-1639812165204</coId><source>https://upload-images.jianshu.io/upload_images/1409498-de9187edf7470328.png?imageMogr2/auto-orient/strip|imageView2/2/w/557/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>20-1639812165204</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>72-1639812165260</coId><text/><inline-styles/><styles/></para><para><coId>87-1639812165205</coId><text>2、在分类回归阶段，在R-CNN中，先生成候选框，然后再通过CNN提取特征，之后再用SVM分类，最后再做回归得到具体位置（bbox regression）。而在Fast R-CNN中，作者巧妙的把最后的bbox regression也放进了神经网络内部，与区域分类合并成为了一个multi-task模型，如下图所示：</text><inline-styles><font-size><from>0</from><to>158</to><value>16</value></font-size><color><from>0</from><to>158</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>39-1639812165207</coId><source>https://upload-images.jianshu.io/upload_images/1409498-688be011e8a47ea4.png?imageMogr2/auto-orient/strip|imageView2/2/w/768/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>90-1639812165208</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>19-1639812165260</coId><text/><inline-styles/><styles/></para><para><coId>88-1639812165208</coId><text>实验表明，这两个任务能够共享卷积特征，并且相互促进。</text><inline-styles><font-size><from>0</from><to>26</to><value>16</value></font-size><color><from>0</from><to>26</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>53-1639812165260</coId><text/><inline-styles/><styles/></para><para><coId>79-1639812165209</coId><text>Fast R-CNN很重要的一个贡献是成功地让人们看到了Region Proposal+CNN（候选区域+卷积神经网络）这一框架实时检测的希望，原来多类检测真的可以在保证准确率的同时提升处理速度。</text><inline-styles><font-size><from>0</from><to>98</to><value>16</value></font-size><color><from>0</from><to>98</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>94-1639812165260</coId><text/><inline-styles/><styles/></para><para><coId>51-1639812165210</coId><text>三、Faster R-CNN更快更强</text><inline-styles><font-size><from>0</from><to>18</to><value>16</value></font-size><color><from>0</from><to>18</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>91-1639812165260</coId><text/><inline-styles/><styles/></para><para><coId>84-1639812165211</coId><text>继2014年推出R-CNN，2015年推出Fast R-CNN之后，目标检测界的领军人物Ross Girshick团队在2015年又推出一力作：Faster R-CNN，使简单网络目标检测速度达到17fps，在PASCAL VOC上准确率为59.9%，复杂网络达到5fps，准确率78.8%。</text><inline-styles><font-size><from>0</from><to>146</to><value>16</value></font-size><color><from>0</from><to>146</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>40-1639812165261</coId><text/><inline-styles/><styles/></para><para><coId>87-1639812165213</coId><text>在Fast R-CNN还存在着瓶颈问题：Selective Search（选择性搜索）。要找出所有的候选框，这个也非常耗时。那我们有没有一个更加高效的方法来求出这些候选框呢？</text><inline-styles><font-size><from>0</from><to>87</to><value>16</value></font-size><color><from>0</from><to>87</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>51-1639812165261</coId><text/><inline-styles/><styles/></para><para><coId>61-1639812165214</coId><text>在Faster R-CNN中加入一个提取边缘的神经网络，也就说找候选框的工作也交给神经网络来做了。这样，目标检测的四个基本步骤（候选区域生成，特征提取，分类，位置精修）终于被统一到一个深度网络框架之内。如下图所示：</text><inline-styles><font-size><from>0</from><to>107</to><value>16</value></font-size><color><from>0</from><to>107</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>22-1639812165215</coId><source>https://upload-images.jianshu.io/upload_images/1409498-f4f48d7b65239f26.png?imageMogr2/auto-orient/strip|imageView2/2/w/752/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>68-1639812165216</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>24-1639812165261</coId><text/><inline-styles/><styles/></para><para><coId>16-1639812165216</coId><text>Faster R-CNN可以简单地看成是“区域生成网络+Fast R-CNN”的模型，用区域生成网络（Region Proposal Network，简称RPN）来代替Fast R-CNN中的Selective Search（选择性搜索）方法。</text><inline-styles><font-size><from>0</from><to>122</to><value>16</value></font-size><color><from>0</from><to>122</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>41-1639812165261</coId><text/><inline-styles/><styles/></para><para><coId>97-1639812165218</coId><text>如下图</text><inline-styles><font-size><from>0</from><to>3</to><value>16</value></font-size><color><from>0</from><to>3</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>91-1639812165218</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>23-1639812165219</coId><source>https://upload-images.jianshu.io/upload_images/1409498-5f06f9802daeaea1.png?imageMogr2/auto-orient/strip|imageView2/2/w/537/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>71-1639812165219</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>54-1639812165261</coId><text/><inline-styles/><styles/></para><para><coId>85-1639812165219</coId><text>RPN如下图：</text><inline-styles><font-size><from>0</from><to>7</to><value>16</value></font-size><color><from>0</from><to>7</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>12-1639812165219</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>15-1639812165220</coId><source>https://upload-images.jianshu.io/upload_images/1409498-80e7bba2eb2f4cb4.png?imageMogr2/auto-orient/strip|imageView2/2/w/601/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>43-1639812165220</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>12-1639812165261</coId><text/><inline-styles/><styles/></para><para><coId>25-1639812165220</coId><text>RPN的工作步骤如下：</text><inline-styles><font-size><from>0</from><to>11</to><value>16</value></font-size><color><from>0</from><to>11</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><list-item level="1" list-id="67-1639812165221"><coId>08-1639812165221</coId><text>在feature map（特征图）上滑动窗口</text><inline-styles><font-size><from>0</from><to>22</to><value>16</value></font-size><color><from>0</from><to>22</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height></styles></list-item><list-item level="1" list-id="67-1639812165221"><coId>37-1639812165221</coId><text>建一个神经网络用于物体分类+框位置的回归</text><inline-styles><font-size><from>0</from><to>20</to><value>16</value></font-size><color><from>0</from><to>20</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height></styles></list-item><list-item level="1" list-id="67-1639812165221"><coId>18-1639812165222</coId><text>滑动窗口的位置提供了物体的大体位置信息</text><inline-styles><font-size><from>0</from><to>19</to><value>16</value></font-size><color><from>0</from><to>19</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height></styles></list-item><list-item level="1" list-id="67-1639812165221"><coId>39-1639812165223</coId><text>框的回归提供了框更精确的位置</text><inline-styles><font-size><from>0</from><to>14</to><value>16</value></font-size><color><from>0</from><to>14</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height></styles></list-item><para><coId>22-1639812165223</coId><text>Faster R-CNN设计了提取候选区域的网络RPN，代替了费时的Selective Search（选择性搜索），使得检测速度大幅提升，下表对比了R-CNN、Fast R-CNN、Faster R-CNN的检测速度：</text><inline-styles><font-size><from>0</from><to>109</to><value>16</value></font-size><color><from>0</from><to>109</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>17-1639812165224</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>47-1639812165225</coId><source>https://upload-images.jianshu.io/upload_images/1409498-c03322ccd893f58e.png?imageMogr2/auto-orient/strip|imageView2/2/w/606/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>01-1639812165225</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>53-1639812165261</coId><text/><inline-styles/><styles/></para><para><coId>17-1639812165225</coId><text>总结</text><inline-styles><font-size><from>0</from><to>2</to><value>16</value></font-size><color><from>0</from><to>2</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>40-1639812165261</coId><text/><inline-styles/><styles/></para><para><coId>70-1639812165225</coId><text>R-CNN、Fast R-CNN、Faster R-CNN一路走来，基于深度学习目标检测的流程变得越来越精简、精度越来越高、速度也越来越快。基于region proposal（候选区域）的R-CNN系列目标检测方法是目标检测技术领域中的最主要分支之一。</text><inline-styles><font-size><from>0</from><to>126</to><value>16</value></font-size><color><from>0</from><to>126</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>53-1639812165262</coId><text/><inline-styles/><styles/></para><para><coId>77-1639812165227</coId><text>为了更加精确地识别目标，实现在像素级场景中识别不同目标，利用“图像分割”技术定位每个目标的精确像素，如下图所示（精确分割出人、汽车、红绿灯等）：</text><inline-styles><font-size><from>0</from><to>72</to><value>16</value></font-size><color><from>0</from><to>72</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>35-1639812165228</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><image><coId>60-1639812165228</coId><source>https://upload-images.jianshu.io/upload_images/1409498-788e884b1dc27488.png?imageMogr2/auto-orient/strip|imageView2/2/w/558/format/webp</source><text/><styles><align>center</align></styles></image><para><coId>47-1639812165229</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>20-1639812165262</coId><text/><inline-styles/><styles/></para><para><coId>02-1639812165229</coId><text>Mask R-CNN便是这种“图像分割”的重要模型。</text><inline-styles><font-size><from>0</from><to>26</to><value>16</value></font-size><color><from>0</from><to>26</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>08-1639812165262</coId><text/><inline-styles/><styles/></para><para><coId>83-1639812165229</coId><text>Mask R-CNN的思路很简洁，既然Faster R-CNN目标检测的效果非常好，每个候选区域能输出种类标签和定位信息，那么就在Faster R-CNN的基础上再添加一个分支从而增加一个输出，即物体掩膜（object mask），也即由原来的两个任务（分类+回归）变为了三个任务（分类+回归+分割）。如下图所示，Mask R-CNN由两条分支组成：</text><inline-styles><font-size><from>0</from><to>175</to><value>16</value></font-size><color><from>0</from><to>175</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>89-1639812165231</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>65-1639812165232</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>42-1639812165262</coId><text/><inline-styles/><styles/></para><para><coId>06-1639812165232</coId><text>Mask R-CNN的这两个分支是并行的，因此训练简单，仅比Faster R-CNN多了一点计算开销。</text><inline-styles><font-size><from>0</from><to>51</to><value>16</value></font-size><color><from>0</from><to>51</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>18-1639812165262</coId><text/><inline-styles/><styles/></para><para><coId>12-1639812165233</coId><text>如下图所示，Mask R-CNN在Faster R-CNN中添加了一个全卷积网络的分支（图中白色部分），用于输出二进制mask，以说明给定像素是否是目标的一部分。所谓二进制mask，就是当像素属于目标的所有位置上时标识为1，其它位置标识为 0</text><inline-styles><font-size><from>0</from><to>121</to><value>16</value></font-size><color><from>0</from><to>121</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>80-1639812165234</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>02-1639812165235</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>99-1639812165262</coId><text/><inline-styles/><styles/></para><para><coId>37-1639812165236</coId><text>从上图可以看出，二进制mask是基于特征图输出的，而原始图像经过一系列的卷积、池化之后，尺寸大小已发生了多次变化，如果直接使用特征图输出的二进制mask来分割图像，那肯定是不准的。这时就需要进行了修正，也即使用RoIAlign替换RoIPooling</text><inline-styles><font-size><from>0</from><to>125</to><value>16</value></font-size><color><from>0</from><to>125</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>01-1639812165237</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>42-1639812165238</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>52-1639812165262</coId><text/><inline-styles/><styles/></para><para><coId>86-1639812165238</coId><text>如上图所示，原始图像尺寸大小是128x128，经过卷积网络之后的特征图变为尺寸大小变为 25x25。这时，如果想要圈出与原始图像中左上方15x15像素对应的区域，那么如何在特征图中选择相对应的像素呢？</text><inline-styles><font-size><from>0</from><to>100</to><value>16</value></font-size><color><from>0</from><to>100</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>35-1639812165262</coId><text/><inline-styles/><styles/></para><para><coId>51-1639812165239</coId><text>从上面两张图可以看出，原始图像中的每个像素对应于特征图的25/128像素，因此，要从原始图像中选择15x15像素，则只需在特征图中选择2.93x2.93像素（15x25/128=2.93），在RoIAlign中会使用双线性插值法准确得到2.93像素的内容，这样就能很大程度上，避免了错位问题。</text><inline-styles><font-size><from>0</from><to>146</to><value>16</value></font-size><color><from>0</from><to>146</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>00-1639812165263</coId><text/><inline-styles/><styles/></para><para><coId>99-1639812165241</coId><text>修改后的网络结构如下图所示（黑色部分为原来的Faster R-CNN，红色部分为Mask R-CNN修改的部分）</text><inline-styles><font-size><from>0</from><to>56</to><value>16</value></font-size><color><from>0</from><to>56</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>67-1639812165242</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>41-1639812165242</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>36-1639812165263</coId><text/><inline-styles/><styles/></para><para><coId>48-1639812165242</coId><text>从上图可以看出损失函数变为</text><inline-styles><font-size><from>0</from><to>13</to><value>16</value></font-size><color><from>0</from><to>13</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>25-1639812165243</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>07-1639812165243</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>41-1639812165263</coId><text/><inline-styles/><styles/></para><para><coId>13-1639812165243</coId><text>损失函数为分类误差+检测误差+分割误差，分类误差和检测（回归）误差是Faster R-CNN中的，分割误差为Mask R-CNN中新加的。</text><inline-styles><font-size><from>0</from><to>69</to><value>16</value></font-size><color><from>0</from><to>69</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>26-1639812165263</coId><text/><inline-styles/><styles/></para><para><coId>78-1639812165244</coId><text>对于每个MxM大小的ROI区域，mask分支有KxMxM维的输出（K是指类别数量）。对于每一个像素，都是用sigmod函数求二值交叉熵，也即对每个像素都进行逻辑回归，得到平均的二值交叉熵误差Lmask。通过引入预测K个输出的机制，允许每个类都生成独立的mask，以避免类间竞争，这样就能解耦mask和种类预测。</text><inline-styles><font-size><from>0</from><to>155</to><value>16</value></font-size><color><from>0</from><to>155</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>77-1639812165263</coId><text/><inline-styles/><styles/></para><para><coId>63-1639812165246</coId><text>对于每一个ROI区域，如果检测得到属于哪一个分类，就只使用该类的交叉熵误差进行计算，也即对于一个ROI区域中KxMxM的输出，真正有用的只是某个类别的MxM的输出。如下图所示：</text><inline-styles><font-size><from>0</from><to>88</to><value>16</value></font-size><color><from>0</from><to>88</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>32-1639812165247</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>67-1639812165247</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>78-1639812165263</coId><text/><inline-styles/><styles/></para><para><coId>66-1639812165248</coId><text>例如目前有3个分类：猫、狗、人，检测得到当前ROI属于“人”这一类，那么所使用的Lmask为“人”这一分支的mask。</text><inline-styles><font-size><from>0</from><to>59</to><value>16</value></font-size><color><from>0</from><to>59</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>57-1639812165263</coId><text/><inline-styles/><styles/></para><para><coId>52-1639812165248</coId><text>Mask R-CNN将这些二进制mask与来自Faster R-CNN的分类和边界框组合，便产生了惊人的图像精确分割，如下图所示：</text><inline-styles><font-size><from>0</from><to>65</to><value>16</value></font-size><color><from>0</from><to>65</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>94-1639812165249</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>46-1639812165250</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>44-1639812165263</coId><text/><inline-styles/><styles/></para><para><coId>48-1639812165250</coId><text>Mask R-CNN是一个小巧、灵活的通用对象实例分割框架，它不仅可以对图像中的目标进行检测，还可以对每一个目标输出一个高质量的分割结果。另外，Mask R-CNN还易于泛化到其他任务，比如人物关键点检测，如下图所示：</text><inline-styles><font-size><from>0</from><to>109</to><value>16</value></font-size><color><from>0</from><to>109</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>23-1639812165251</coId><text/><inline-styles/><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>76-1639812165252</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para><para><coId>26-1639812165263</coId><text/><inline-styles/><styles/></para><para><coId>40-1639812165252</coId><text>从R-CNN、Fast R-CNN、Faster R-CNN到Mask R-CNN，每次进步不一定是跨越式的发展，这些进步实际上是直观的且渐进的改进之路，但是它们的总和却带来了非常显著的效果。</text><inline-styles><font-size><from>0</from><to>96</to><value>16</value></font-size><color><from>0</from><to>96</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>20-1639812165263</coId><text/><inline-styles/><styles/></para><para><coId>18-1639812165253</coId><text>最后，总结一下目标检测算法模型的发展历程，如下图所示：</text><inline-styles><font-size><from>0</from><to>27</to><value>16</value></font-size><color><from>0</from><to>27</to><value>#404040</value></color></inline-styles><styles><line-height>1.8</line-height><margin>0px 0px 20px</margin><font-size>16</font-size></styles></para><para><coId>35-1639812165263</coId><text/><inline-styles/><styles/></para><para><coId>09-1639812165254</coId><text>image</text><inline-styles><font-size><from>0</from><to>5</to><value>13</value></font-size><color><from>0</from><to>5</to><value>#999999</value></color></inline-styles><styles><align>center</align><line-height>1.7999999999999998</line-height><margin>0px 0px 20px</margin><padding>10px</padding><font-size>13</font-size></styles></para></body></note>